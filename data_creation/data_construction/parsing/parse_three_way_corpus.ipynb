{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import spacy\n",
    "import csv\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import stanza\n",
    "import spacy_stanza\n",
    "import benepar\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from data_construction.parallel_corpus.utils import merge_maximum_span\n",
    "from data_construction.parallel_corpus.utils import clean_sentence_brackets\n",
    "from data_construction.parallel_corpus.utils import process_nps_punctuation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "sm_parser = spacy.load('en_core_web_sm')\n",
    "berkeley_parser = spacy.load('en_core_web_md')\n",
    "berkeley_parser.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "trf_parser = spacy.load(\"en_core_web_trf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[It, the morning]\n",
      "[It, the morning]\n",
      "[It, the morning]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyuanzheng/.conda/envs/multi_coref/lib/python3.8/site-packages/torch/distributions/distribution.py:44: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n",
      "/Users/boyuanzheng/.conda/envs/multi_coref/lib/python3.8/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "sentence = \"It is 5:30am in the morning.\"\n",
    "\n",
    "print([item for item in sm_parser(sentence).noun_chunks])\n",
    "print([item for item in berkeley_parser(sentence).noun_chunks])\n",
    "print([item for item in trf_parser(sentence).noun_chunks])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d12a6248a8b84e4a8a0b3e20ea53f1ba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 13:28:09 INFO: Downloading default packages for language: en (English)...\n",
      "2022-05-17 13:28:11 INFO: File exists: /Users/boyuanzheng/stanza_resources/en/default.zip.\n",
      "2022-05-17 13:28:15 INFO: Finished downloading models and saved to /Users/boyuanzheng/stanza_resources.\n",
      "2022-05-17 13:28:15 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-05-17 13:28:15 INFO: Use device: cpu\n",
      "2022-05-17 13:28:15 INFO: Loading: tokenize\n",
      "2022-05-17 13:28:15 INFO: Loading: pos\n",
      "2022-05-17 13:28:16 INFO: Loading: lemma\n",
      "2022-05-17 13:28:16 INFO: Loading: depparse\n",
      "2022-05-17 13:28:16 INFO: Loading: sentiment\n",
      "2022-05-17 13:28:16 INFO: Loading: constituency\n",
      "2022-05-17 13:28:17 INFO: Loading: ner\n",
      "2022-05-17 13:28:18 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download(\"en\")\n",
    "stanza_parser = spacy_stanza.load_pipeline(\"en\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[It]\n"
     ]
    }
   ],
   "source": [
    "print([item for item in stanza_parser(sentence).noun_chunks])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stanza_parser."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "source_data = []\n",
    "with open('../parallel_corpus/parallel_data/en_fa_zh_parallel_corpus.json', 'r') as f:\n",
    "    reader = json.load(f)\n",
    "    for x in reader:\n",
    "        source_data.append(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16252 [00:00<?, ?it/s]/Users/boyuanzheng/.conda/envs/multi_coref/lib/python3.8/site-packages/torch/distributions/distribution.py:44: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n",
      "/Users/boyuanzheng/.conda/envs/multi_coref/lib/python3.8/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "100%|██████████| 16252/16252 [1:20:19<00:00,  3.37it/s]  \n"
     ]
    }
   ],
   "source": [
    "parsed_data = []\n",
    "for instance in tqdm(source_data):\n",
    "    text = instance['en_utterance']\n",
    "    utterance = sm_parser(text)\n",
    "    instance['sm_noun_chunk'] = [(item.text, item.start, item.end) for item in utterance.noun_chunks]\n",
    "    instance['sm_pron'] = [(item.text,i, i+1, item.pos_, item.tag_) for i, item in enumerate(utterance)]\n",
    "\n",
    "    utterance = berkeley_parser(text)\n",
    "    instance['berkeley_noun_chunk'] = [(item.text, item.start, item.end) for item in utterance.noun_chunks]\n",
    "    instance['berkeley_pron'] = [(item.text,i, i+1, item.pos_, item.tag_) for i, item in enumerate(utterance)]\n",
    "\n",
    "    utterance = trf_parser(text)\n",
    "    instance['trf_noun_chunk'] = [(item.text, item.start, item.end) for item in utterance.noun_chunks]\n",
    "    instance['trf_pron'] = [(item.text,i, i+1, item.pos_, item.tag_) for i, item in enumerate(utterance)]\n",
    "    parsed_data.append(instance)\n",
    "\n",
    "with open('parsed_three_way_corpus.pkl', 'wb') as f:\n",
    "    pkl.dump(parsed_data, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "with open('parsed_three_way_corpus.pkl', 'rb') as f:\n",
    "    temp = pkl.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en_utterance': \"That was the closest I've come to sex in, like, two years.\", 'fa_utterance': 'اين نزديکترين برخوردي بود که احتمال سکس داشت تو دو سال گذشته', 'zh_utterance': '谢谢 这大概是我两年来 最接近滚床单的时刻了', 'sm_noun_chunk': [('That', 0, 1), ('I', 4, 5), ('sex', 8, 9)], 'sm_pron': [('That', 0, 1, 'PRON', 'DT'), ('was', 1, 2, 'AUX', 'VBD'), ('the', 2, 3, 'DET', 'DT'), ('closest', 3, 4, 'ADJ', 'JJS'), ('I', 4, 5, 'PRON', 'PRP'), (\"'ve\", 5, 6, 'AUX', 'VBP'), ('come', 6, 7, 'VERB', 'VBN'), ('to', 7, 8, 'ADP', 'IN'), ('sex', 8, 9, 'NOUN', 'NN'), ('in', 9, 10, 'ADP', 'RP'), (',', 10, 11, 'PUNCT', ','), ('like', 11, 12, 'INTJ', 'UH'), (',', 12, 13, 'PUNCT', ','), ('two', 13, 14, 'NUM', 'CD'), ('years', 14, 15, 'NOUN', 'NNS'), ('.', 15, 16, 'PUNCT', '.')], 'berkeley_noun_chunk': [('That', 0, 1), ('I', 4, 5), ('sex', 8, 9)], 'berkeley_pron': [('That', 0, 1, 'PRON', 'DT'), ('was', 1, 2, 'AUX', 'VBD'), ('the', 2, 3, 'DET', 'DT'), ('closest', 3, 4, 'ADJ', 'JJS'), ('I', 4, 5, 'PRON', 'PRP'), (\"'ve\", 5, 6, 'AUX', 'VBP'), ('come', 6, 7, 'VERB', 'VBN'), ('to', 7, 8, 'ADP', 'IN'), ('sex', 8, 9, 'NOUN', 'NN'), ('in', 9, 10, 'ADV', 'IN'), (',', 10, 11, 'PUNCT', ','), ('like', 11, 12, 'INTJ', 'IN'), (',', 12, 13, 'PUNCT', ','), ('two', 13, 14, 'NUM', 'CD'), ('years', 14, 15, 'NOUN', 'NNS'), ('.', 15, 16, 'PUNCT', '.')], 'trf_noun_chunk': [('That', 0, 1), ('I', 4, 5), ('sex', 8, 9), ('two years', 13, 15)], 'trf_pron': [('That', 0, 1, 'PRON', 'DT'), ('was', 1, 2, 'AUX', 'VBD'), ('the', 2, 3, 'DET', 'DT'), ('closest', 3, 4, 'ADJ', 'JJS'), ('I', 4, 5, 'PRON', 'PRP'), (\"'ve\", 5, 6, 'AUX', 'VBP'), ('come', 6, 7, 'VERB', 'VBN'), ('to', 7, 8, 'ADP', 'IN'), ('sex', 8, 9, 'NOUN', 'NN'), ('in', 9, 10, 'ADP', 'IN'), (',', 10, 11, 'PUNCT', ','), ('like', 11, 12, 'INTJ', 'UH'), (',', 12, 13, 'PUNCT', ','), ('two', 13, 14, 'NUM', 'CD'), ('years', 14, 15, 'NOUN', 'NNS'), ('.', 15, 16, 'PUNCT', '.')]}\n"
     ]
    }
   ],
   "source": [
    "print(temp[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "[('her', 13, 14),\n ('anyone', 8, 9),\n ('my wife', 4, 6),\n ('sex', 17, 18),\n ('it', 19, 20),\n ('me', 21, 22),\n ('that', 2, 3),\n ('my', 4, 5)]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collect_all_mentions(instance):\n",
    "    sentence_token = [item[0] for item in instance['sm_pron']]\n",
    "    sm_nps = process_nps_punctuation(sentence_token, process_nps_punctuation(sentence_token, instance['sm_noun_chunk']))\n",
    "    berkeley_nps = process_nps_punctuation(sentence_token, process_nps_punctuation(sentence_token, instance['berkeley_noun_chunk']))\n",
    "    trf_nps = process_nps_punctuation(sentence_token, process_nps_punctuation(sentence_token, instance['trf_noun_chunk']))\n",
    "    noun_phrase = merge_maximum_span(list(set(sm_nps) | set(berkeley_nps) | set(trf_nps)))\n",
    "    temp_pron = []\n",
    "    temp_pron.extend([(item[0], item[1], item[2]) for item in instance['sm_pron'] if item[3]=='PRON'])\n",
    "    temp_pron.extend([(item[0], item[1], item[2]) for item in instance['berkeley_pron'] if item[3]=='PRON'])\n",
    "    temp_pron.extend([(item[0], item[1], item[2]) for item in instance['trf_pron'] if item[3]=='PRON'])\n",
    "    pron = merge_maximum_span(list(set(temp_pron)))\n",
    "    all = list(set(noun_phrase) | set(pron))\n",
    "    return all\n",
    "collect_all_mentions(temp[3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2504/2504 [00:00<00:00, 9775.72it/s]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for item in tqdm(temp[-2504:]):\n",
    "    try:\n",
    "        count += len(collect_all_mentions(item))\n",
    "    except:\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16252\n"
     ]
    }
   ],
   "source": [
    "print(len(temp))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11980\n"
     ]
    }
   ],
   "source": [
    "print(count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}