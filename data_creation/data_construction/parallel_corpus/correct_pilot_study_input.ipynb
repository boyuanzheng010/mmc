{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1131072"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import spacy\n",
    "import csv\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import benepar\n",
    "csv.field_size_limit(1131072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from utils import merge_maximum_span\n",
    "from utils import process_nps_punctuation\n",
    "from utils import combine_samples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "sm_parser = spacy.load('en_core_web_sm')\n",
    "berkeley_parser = spacy.load('en_core_web_md')\n",
    "berkeley_parser.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "trf_parser = spacy.load(\"en_core_web_trf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/person_name_1zheng/.conda/envs/multi_coref/lib/python3.8/site-packages/torch/distributions/distribution.py:44: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n",
      "/Users/person_name_1zheng/.conda/envs/multi_coref/lib/python3.8/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "with open('pilot_data/pilot_1_source.csv', \"r\", encoding=\"utf-8\") as csv_fh:\n",
    "    reader = csv.DictReader(csv_fh)\n",
    "    output = []\n",
    "    for line in reader:\n",
    "        sentences = json.loads(line['json_data'])['sentences']\n",
    "        all_sentences = []\n",
    "        all_query_spans = []\n",
    "        j = 0\n",
    "        for sent in sentences:\n",
    "            speaker = sent[0].strip()\n",
    "            sentence = \" \".join(sent[2:]).strip()\n",
    "            utt = {}\n",
    "            utt['sm_noun_chunk'] = [(item.text, item.start, item.end) for item in sm_parser(sentence).noun_chunks]\n",
    "            utt['sm_noun_chunk'] = utt['sm_noun_chunk'] + [(item.text,i, i+1) for i, item in enumerate(sm_parser(sentence)) if item.pos_==\"PROPN\"]\n",
    "            utt['sm_pron'] = [(item.text,i, i+1) for i, item in enumerate(sm_parser(sentence)) if item.pos_==\"PRON\"]\n",
    "\n",
    "            utt['berkeley_noun_chunk'] = [(item.text, item.start, item.end) for item in berkeley_parser(sentence).noun_chunks]\n",
    "            utt['berkeley_noun_chunk'] = utt['berkeley_noun_chunk'] + [(item.text,i, i+1) for i, item in enumerate(berkeley_parser(sentence)) if item.pos_==\"PROPN\"]\n",
    "            utt['berkeley_pron'] = [(item.text,i, i+1) for i, item in enumerate(berkeley_parser(sentence)) if item.pos_==\"PRON\"]\n",
    "\n",
    "            utt['trf_noun_chunk'] = [(item.text, item.start, item.end) for item in trf_parser(sentence).noun_chunks]\n",
    "            utt['trf_noun_chunk'] = utt['trf_noun_chunk'] + [(item.text,i, i+1) for i, item in enumerate(trf_parser(sentence)) if item.pos_==\"PROPN\"]\n",
    "            utt['trf_pron'] = [(item.text,i, i+1) for i, item in enumerate(trf_parser(sentence)) if item.pos_==\"PRON\"]\n",
    "\n",
    "            sentence_token = [item.text for item in sm_parser(sentence)]\n",
    "            noun_phrase = merge_maximum_span(list(set(utt['sm_noun_chunk']) | set(utt['berkeley_noun_chunk']) | set(utt['trf_noun_chunk'])))\n",
    "            noun_phrase = process_nps_punctuation(sentence_token, process_nps_punctuation(sentence_token, noun_phrase))\n",
    "\n",
    "            pron = merge_maximum_span(list(set(utt['sm_pron']) | set(utt['berkeley_pron']) | set(utt['trf_pron'])))\n",
    "            pron = process_nps_punctuation(sentence_token, process_nps_punctuation(sentence_token, pron))\n",
    "\n",
    "            mention = list(set(noun_phrase)|set(pron))\n",
    "            mention.sort(key=lambda x: x[1])\n",
    "\n",
    "            speaker_tokens = [speaker]\n",
    "            all_sentences.append([speaker_tokens] + [\":\"] + sentence_token)\n",
    "            for span in mention:\n",
    "                all_query_spans.append({\n",
    "                        \"sentenceIndex\": j,\n",
    "                        \"startToken\": span[1] + len(speaker_tokens) + 1,\n",
    "                        \"endToken\": span[2] + len(speaker_tokens) + 1\n",
    "                    })\n",
    "            j+=1\n",
    "        output.append({\n",
    "            \"sentences\": all_sentences,\n",
    "            \"querySpans\": all_query_spans,\n",
    "            \"candidateSpans\": all_query_spans,\n",
    "            \"clickSpans\": all_query_spans,\n",
    "            })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "combined_output = []\n",
    "combined_output.append(combine_samples(output))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "with open('pilot_data/corrected_pilot_1_combined.csv', \"w\", encoding=\"utf-8\") as csv_fh:\n",
    "        fieldnames = ['json_data']\n",
    "        writer = csv.DictWriter(csv_fh, fieldnames, lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        for line in combined_output:\n",
    "            writer.writerow({'json_data': json.dumps(line)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "with open('pilot_data/corrected_pilot_1.csv', \"w\", encoding=\"utf-8\") as csv_fh:\n",
    "        fieldnames = ['json_data']\n",
    "        writer = csv.DictWriter(csv_fh, fieldnames, lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        for line in output:\n",
    "            writer.writerow({'json_data': json.dumps(line)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}