{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nIn this script, we want to convert annotations to conll style. There are several steps:\\n1.clustering input data\\n2.Turn cluster data into conll format\\n3.Try to run the preprocess script from HOI code base\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this script, we want to convert annotations to conll style. There are several steps:\n",
    "1.Clustering input data\n",
    "2.Turn cluster data into conll format\n",
    "3.Try to run the preprocess script from HOI code base\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from copy import deepcopy\n",
    "import jsonlines\n",
    "from utils.my_util import cluster_mentions, remove_speaker_prefix\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Dialogue Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "{'0': {'train': [0, 1, 2], 'val': [3], 'test': [4]},\n '1': {'train': [0, 1, 2], 'val': [3], 'test': [4]}}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"0\": {\n",
    "        \"train\": [0, 1, 2],\n",
    "        \"val\": [3],\n",
    "        \"test\": [4]\n",
    "    },\n",
    "    \"1\": {\n",
    "        \"train\": [0, 1, 2],\n",
    "        \"val\": [3],\n",
    "        \"test\": [4]\n",
    "    },\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "python train_data.py --set=0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "speaker_dict = {}\n",
    "with open('data/raw_source/dialogue_zh/all_coref_data_en.json', 'r') as f:\n",
    "    temp = json.load(f)\n",
    "    for line in temp:\n",
    "        scene_id = line['scene_id']\n",
    "        speakers = []\n",
    "        for sent in line['sentences']:\n",
    "            speakers.append(\" \".join(sent[:sent.index(\":\")]))\n",
    "        speaker_dict[scene_id] = speakers\n",
    "\n",
    "split_dict = {\"train\":[], \"dev\":[], \"test\":[]}\n",
    "with open('data/raw_source/dialogue_zh/dev_temp.pkl', 'rb') as f:\n",
    "    temp = pkl.load(f)\n",
    "    for line in temp:\n",
    "        split_dict['dev'].append(line['scene_id']+\"0\")\n",
    "with open('data/raw_source/dialogue_zh/test_temp.pkl', 'rb') as f:\n",
    "    temp = pkl.load(f)\n",
    "    for line in temp:\n",
    "        split_dict['test'].append(line['scene_id']+\"0\")\n",
    "with open('data/raw_source/dialogue_zh/train_temp.pkl', 'rb') as f:\n",
    "    temp = pkl.load(f)\n",
    "    for line in temp:\n",
    "        split_dict['train'].append(line['scene_id'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def remove_empty_sentences(instance):\n",
    "    sentences = instance['sentences']\n",
    "    answers = instance['answers']\n",
    "    speakers = instance['speakers']\n",
    "\n",
    "    # Build old sent_id to new sent_id map\n",
    "    map_sent_id = {}\n",
    "    count = 0\n",
    "    for i, sent in enumerate(sentences):\n",
    "        if sent == []:\n",
    "            continue\n",
    "        map_sent_id[i] = count\n",
    "        count += 1\n",
    "\n",
    "    # Collect answers, speakers for each sentence\n",
    "    temp = []\n",
    "    for i, sent in enumerate(sentences):\n",
    "        if sent == []:\n",
    "            continue\n",
    "        annotations = []\n",
    "        for answer in answers:\n",
    "            if answer[0][0]==i:\n",
    "                annotations.append(answer)\n",
    "        temp.append([sent, annotations, speakers[i]])\n",
    "\n",
    "    # Change Sentence ID\n",
    "    sentences = []\n",
    "    answers = []\n",
    "    speakers = []\n",
    "    for i, (sent, annotations, speaker) in enumerate(temp):\n",
    "        # print(i, speaker, sent)\n",
    "        sentences.append(sent)\n",
    "        temp_answers = []\n",
    "        for query, antecedents in annotations:\n",
    "            new_query = tuple((map_sent_id[query[0]], query[1], query[2], query[3]))\n",
    "            # print(query, new_query)\n",
    "            new_antecedents = []\n",
    "            if isinstance(antecedents, str):\n",
    "                new_antecedents = antecedents\n",
    "                # print(new_antecedents)\n",
    "            else:\n",
    "                # print(antecedents)\n",
    "                for antecedent in antecedents:\n",
    "                    new_antecedents.append((map_sent_id[antecedent[0]], antecedent[1], antecedent[2], antecedent[3]))\n",
    "            # print(new_antecedents)\n",
    "            temp_answers.append([new_query, new_antecedents])\n",
    "        answers.extend(temp_answers)\n",
    "        speakers.append(speaker)\n",
    "\n",
    "    return {\n",
    "        \"sentences\": sentences,\n",
    "        \"answers\": answers,\n",
    "        \"speakers\": speakers,\n",
    "        \"scene_id\": instance['scene_id']\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cluster_mentions(answers, sentences):\n",
    "    \"\"\"\n",
    "    Cluster mention including plural. The clustering steps are as follows:\n",
    "    1. Gather all non-plural query-annotation pairs\n",
    "    2. Merge all non-plural pairs to build big cluster\n",
    "    3. Add Plural: turn each mention in plural into {1.speaker name, 2.cluster id, 3.turn sent_id, start, end into special identification}, then use these to build a string for clustering\n",
    "    4. Add each pair to cluster and do merging\n",
    "    5. Remove strings from each cluster\n",
    "    ?One Thing to Consider: Whether use speaker mention in sentence to merge to speaker cluster?\n",
    "    \"\"\"\n",
    "    all_clusters = []\n",
    "    speaker_set = set()\n",
    "    # Generate all cluster (no plural), we will add plural latter\n",
    "    for query, annotations in answers:\n",
    "        if isinstance(annotations, str):\n",
    "            if annotations==\"notPresent\":\n",
    "                all_clusters.append([query])\n",
    "        elif len(annotations)==1:\n",
    "            temp = [query]\n",
    "            for token in annotations:\n",
    "                if token[1]==0:\n",
    "                    try:\n",
    "                        speaker = \" \".join(sentences[token[0]][token[1]: sentences[token[0]].index(\":\")]).lower()\n",
    "                        temp.append(speaker)\n",
    "                        speaker_set.add(speaker)\n",
    "                    except:\n",
    "                        continue\n",
    "                else:\n",
    "                    temp.append(token)\n",
    "            all_clusters.append(temp)\n",
    "\n",
    "    # Merge clusters if any clusters have common mentions\n",
    "    merged_clusters = []\n",
    "    for cluster in all_clusters:\n",
    "        existing = None\n",
    "        for mention in cluster:\n",
    "            for merged_cluster in merged_clusters:\n",
    "                if mention in merged_cluster:\n",
    "                    existing = merged_cluster\n",
    "                    break\n",
    "            if existing is not None:\n",
    "                break\n",
    "        if existing is not None:\n",
    "            existing.update(cluster)\n",
    "        else:\n",
    "            merged_clusters.append(set(cluster))\n",
    "    merged_clusters = [list(cluster) for cluster in merged_clusters]\n",
    "\n",
    "    # Add Plural\n",
    "    for query, annotations in answers:\n",
    "        if isinstance(annotations, list):\n",
    "            if len(annotations)>1:\n",
    "                temp_anno = []\n",
    "                for token in annotations:\n",
    "                    if token[1]==0:\n",
    "                        speaker = \" \".join(sentences[token[0]][token[1]: sentences[token[0]].index(\":\")]).lower()\n",
    "                        temp_anno.append(speaker)\n",
    "                        speaker_set.add(speaker)\n",
    "                    # if token[1]==0:\n",
    "                    #     try:\n",
    "                    #         speaker = \" \".join(sentences[token[0]][token[1]: sentences[token[0]].index(\":\")]).lower()\n",
    "                    #         temp_anno.append(speaker)\n",
    "                    #         speaker_set.add(speaker)\n",
    "                    #     except:\n",
    "                    #         continue\n",
    "                    else:\n",
    "                        # If the cluster is already in cluster, use the cluster id as identification, else use the index\n",
    "                        cluster_idx = -1\n",
    "                        for idx, cluster in enumerate(merged_clusters):\n",
    "                            if token in cluster:\n",
    "                                cluster_idx = idx\n",
    "                                break\n",
    "                        if cluster_idx != -1:\n",
    "                            temp_anno.append(str(cluster_idx))\n",
    "                        else:\n",
    "                            temp_anno.append(\"*\" + \"*\".join([str(num) for num in token]) + \"*\")\n",
    "                temp_cluster = [query, \"||\".join(sorted(temp_anno))]\n",
    "                merged_clusters.append(temp_cluster)\n",
    "\n",
    "    # Merge Plural\n",
    "    all_clusters = deepcopy(merged_clusters)\n",
    "    merged_clusters = []\n",
    "    for cluster in all_clusters:\n",
    "        existing = None\n",
    "        for mention in cluster:\n",
    "            for merged_cluster in merged_clusters:\n",
    "                if mention in merged_cluster:\n",
    "                    existing = merged_cluster\n",
    "                    break\n",
    "            if existing is not None:\n",
    "                break\n",
    "        if existing is not None:\n",
    "            existing.update(cluster)\n",
    "        else:\n",
    "            merged_clusters.append(set(cluster))\n",
    "    merged_clusters = [list(cluster) for cluster in merged_clusters]\n",
    "\n",
    "    output = []\n",
    "    for cluster in merged_clusters:\n",
    "        output.append([token for token in cluster if isinstance(token, tuple)])\n",
    "\n",
    "    return output\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/w9/673mfrb56v7dvx7hhvw7s6240000gn/T/ipykernel_33736/1805913891.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     34\u001B[0m                             \u001B[0mtemp_answer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\" \"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mantecedents\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m                     \u001B[0manswers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mquery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtemp_answer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m             data.append(remove_empty_sentences({\n\u001B[0m\u001B[1;32m     37\u001B[0m                 \u001B[0;34m\"sentences\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0msentences\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m                 \u001B[0;34m\"answers\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0manswers\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/w9/673mfrb56v7dvx7hhvw7s6240000gn/T/ipykernel_33736/2401979536.py\u001B[0m in \u001B[0;36mremove_empty_sentences\u001B[0;34m(instance)\u001B[0m\n\u001B[1;32m     33\u001B[0m         \u001B[0mtemp_answers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mquery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mantecedents\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mannotations\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 35\u001B[0;31m             \u001B[0mnew_query\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap_sent_id\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mquery\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mquery\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mquery\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mquery\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     36\u001B[0m             \u001B[0;31m# print(query, new_query)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m             \u001B[0mnew_antecedents\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "all_ids = []\n",
    "with open('data/raw_source/dialogue_zh/all_coref_data_en.json', 'r') as f:\n",
    "# with open('data/raw_source/dialogue_zh/dev-test-batch1_zh.json', 'r') as f:\n",
    "    reader = jsonlines.Reader(f)\n",
    "    for bulk in reader:\n",
    "        for idx, instance in enumerate(bulk):\n",
    "            # if idx>=5:\n",
    "            #     break\n",
    "            scene_id = instance['scene_id']\n",
    "            if scene_id == \"\":\n",
    "                continue\n",
    "            sentences = instance['sentences']\n",
    "            # print(sentences)\n",
    "            # sentences = [[token for token in \"\".join(sent)] for sent in instance['sentences']]\n",
    "            annotations = instance['annotations']\n",
    "            all_ids.append(scene_id)\n",
    "            speakers = speaker_dict[scene_id]\n",
    "            answers = []\n",
    "            for item in annotations:\n",
    "                query = (item['query']['sentenceIndex'], item['query']['startToken'], item['query']['endToken'])\n",
    "                antecedents = item['antecedents']\n",
    "                # print(query)\n",
    "                # print(antecedents)\n",
    "                # print()\n",
    "                if antecedents in [['n', 'o', 't', 'P', 'r', 'e', 's', 'e', 'n', 't'], ['null_projection'], ['empty_subtitle']]:\n",
    "                    answers.append([query, \"notPresent\"])\n",
    "                else:\n",
    "                    temp_answer = []\n",
    "                    for antecedent in antecedents:\n",
    "                        if isinstance(antecedent, dict):\n",
    "                            temp_answer.append((antecedent['sentenceIndex'], antecedent['startToken'], antecedent['endToken']))\n",
    "                        else:\n",
    "                            temp_answer = \" \".join(antecedents)\n",
    "                    answers.append([query, temp_answer])\n",
    "            data.append(remove_empty_sentences({\n",
    "                \"sentences\": sentences,\n",
    "                \"answers\": answers,\n",
    "                \"speakers\": speakers,\n",
    "                \"scene_id\": scene_id\n",
    "            }))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "with open('en_mention_id_cluster.pkl', 'rb') as f:\n",
    "    en_mention_id_clusters = pkl.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cluster Chinese Mentions using English-Side Mention_IDs\n",
    "\"\"\"\n",
    "data = []\n",
    "all_ids = []\n",
    "with open('data/raw_source/dialogue_zh/all_coref_data_en_zh_with_id.json', 'r') as f:\n",
    "# with open('data/raw_source/dialogue_zh/dev-test-batch1_zh.json', 'r') as f:\n",
    "    reader = jsonlines.Reader(f)\n",
    "    for bulk in reader:\n",
    "        for idx, instance in enumerate(bulk):\n",
    "            if idx>=10:\n",
    "                break\n",
    "            scene_id = instance['scene_id']\n",
    "            if scene_id == \"\":\n",
    "                continue\n",
    "            sentences = instance['sentences']\n",
    "            # print(sentences)\n",
    "            # sentences = [[token for token in \"\".join(sent)] for sent in instance['sentences']]\n",
    "            annotations = instance['annotations']\n",
    "            all_ids.append(scene_id)\n",
    "            speakers = speaker_dict[scene_id]\n",
    "            answers = []\n",
    "            for item in annotations:\n",
    "                query = (item['query']['sentenceIndex'], item['query']['startToken'], item['query']['endToken'], item['query']['mention_id'])\n",
    "                antecedents = item['antecedents']\n",
    "                if antecedents in [['n', 'o', 't', 'P', 'r', 'e', 's', 'e', 'n', 't'], ['null_projection'], ['empty_subtitle']]:\n",
    "                    answers.append([query, \"notPresent\"])\n",
    "                else:\n",
    "                    temp_answer = []\n",
    "                    for antecedent in antecedents:\n",
    "                        if isinstance(antecedent, dict):\n",
    "                            temp_answer.append((antecedent['sentenceIndex'], antecedent['startToken'], antecedent['endToken'], antecedent['mention_id']))\n",
    "                        else:\n",
    "                            temp_answer = \" \".join(antecedents)\n",
    "                    answers.append([query, temp_answer])\n",
    "            data.append(remove_empty_sentences({\n",
    "                \"sentences\": sentences,\n",
    "                \"answers\": answers,\n",
    "                \"speakers\": speakers,\n",
    "                \"scene_id\": scene_id\n",
    "            }))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correction_dict = {(5, 7, 8): (5, 7, 9), (7, 13, 17): (7, 15, 17), (11, 0, 1): (11, 0, 2), (11, 5, 7): (11, 4, 7), (16, 0, 3): (16, 0, 1)}\n",
    "for item in data:\n",
    "    if item['scene_id'] in ['s07e17c08t1']:\n",
    "        print(item['sentences'])\n",
    "        print()\n",
    "        for token in item['answers']:\n",
    "            print(token)\n",
    "        print()\n",
    "        print(item['speakers'])\n",
    "        print('=='*50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def cluster_mention_by_en_id(answers, sentences):\n",
    "    \"\"\"\n",
    "    We cluster Chinese Side mentions according to the index in English Side\n",
    "    \"\"\"\n",
    "    print(answers)\n",
    "    print()\n",
    "    print(sentences)\n",
    "    print(\"==\"*50)\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "split = \"test\"\n",
    "\n",
    "document = []\n",
    "for i in range(len(data)):\n",
    "    if i >= 2:\n",
    "        continue\n",
    "    sample = data[i]\n",
    "    if sample['scene_id'] not in split_dict[split]:\n",
    "        continue\n",
    "    # print(sample)\n",
    "    # print()\n",
    "    # original_sentences = sample['sentences']\n",
    "    # original_clusters = cluster_mentions(sample['answers'], original_sentences)\n",
    "    # sentences, clusters, speakers = remove_speaker_prefix(original_sentences, original_clusters)\n",
    "    sentences = sample['sentences']\n",
    "    cluster_mention_by_en_id(sample['answers'], sentences)\n",
    "    clusters = cluster_mentions(sample['answers'], sentences)\n",
    "    speakers = sample['speakers']\n",
    "    scene_id = sample['scene_id']\n",
    "    part = int(scene_id[7:9])\n",
    "    begin_line = \"#begin document \" + \"(\" + scene_id + \"); part \" + \"%03d\" % part\n",
    "    end_line = \"#end document\"\n",
    "\n",
    "    # Prepare for clustering\n",
    "    cluster_field = []\n",
    "    for sent in sentences:\n",
    "        cluster_field.append([\"\"]*len(sent))\n",
    "    # Add start\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                # print(cluster_field[sent_id])\n",
    "                # print(sent_id, start, end, len(sentences[sent_id]))\n",
    "                # print(sentences[sent_id])\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx)\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx)\n",
    "    # Add start==end\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start == end:\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx) + \")\"\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx) + \")\"\n",
    "    # Add End\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                try:\n",
    "                    if cluster_field[sent_id][end] == \"\":\n",
    "                        cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                    else:\n",
    "                        cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "                except:\n",
    "                    pass\n",
    "                # if cluster_field[sent_id][end] == \"\":\n",
    "                #     cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                # else:\n",
    "                #     cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "\n",
    "    # Build document\n",
    "    document.append(begin_line + \"\\n\")\n",
    "    for sent, speaker, cluster_value in zip(sentences, speakers, cluster_field):\n",
    "        for j, word in enumerate(sent):\n",
    "            cluster_id = cluster_value[j]\n",
    "            if cluster_id == \"\":\n",
    "                cluster_id = \"-\"\n",
    "            temp = [scene_id, str(part), str(j), word, \"na\", \"na\", \"na\", \"na\", \"na\", speaker, \"na\", \"na\", \"na\", cluster_id]\n",
    "            document.append(\" \".join(temp)+ \"\\n\")\n",
    "        document.append(\"\" + \"\\n\")\n",
    "    document.append(end_line + \"\\n\")\n",
    "\n",
    "# with open(\"data/conll_style/dialogue_chinese/\"+ split+'.chinese.v4_gold_conll', 'w') as f:\n",
    "#     f.writelines(document)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Chinese Sample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1515\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "\n",
    "document = []\n",
    "for i in range(len(data[:1])):\n",
    "    sample = data[i]\n",
    "    if sample['scene_id'] not in split_dict[split]:\n",
    "        continue\n",
    "    # print(sample)\n",
    "    # print()\n",
    "    # original_sentences = sample['sentences']\n",
    "    # original_clusters = cluster_mentions(sample['answers'], original_sentences)\n",
    "    # sentences, clusters, speakers = remove_speaker_prefix(original_sentences, original_clusters)\n",
    "    sentences = sample['sentences']\n",
    "    clusters = cluster_mentions(sample['answers'], sentences)\n",
    "    speakers = sample['speakers']\n",
    "    scene_id = sample['scene_id']\n",
    "    part = int(scene_id[7:9])\n",
    "    begin_line = \"#begin document \" + \"(\" + scene_id + \"); part \" + \"%03d\" % part\n",
    "    end_line = \"#end document\"\n",
    "\n",
    "    # Prepare for clustering\n",
    "    cluster_field = []\n",
    "    for sent in sentences:\n",
    "        cluster_field.append([\"\"]*len(sent))\n",
    "    # Add start\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                # print(cluster_field[sent_id])\n",
    "                # print(sent_id, start, end, len(sentences[sent_id]))\n",
    "                # print(sentences[sent_id])\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx)\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx)\n",
    "    # Add start==end\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start == end:\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx) + \")\"\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx) + \")\"\n",
    "    # Add End\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                try:\n",
    "                    if cluster_field[sent_id][end] == \"\":\n",
    "                        cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                    else:\n",
    "                        cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "                except:\n",
    "                    pass\n",
    "                # if cluster_field[sent_id][end] == \"\":\n",
    "                #     cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                # else:\n",
    "                #     cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "\n",
    "    # Build document\n",
    "    document.append(begin_line + \"\\n\")\n",
    "    for sent, speaker, cluster_value in zip(sentences, speakers, cluster_field):\n",
    "        for j, word in enumerate(sent):\n",
    "            cluster_id = cluster_value[j]\n",
    "            if cluster_id == \"\":\n",
    "                cluster_id = \"-\"\n",
    "            temp = [scene_id, str(part), str(j), word, \"na\", \"na\", \"na\", \"na\", \"na\", speaker, \"na\", \"na\", \"na\", cluster_id]\n",
    "            document.append(\" \".join(temp)+ \"\\n\")\n",
    "        document.append(\"\" + \"\\n\")\n",
    "    document.append(end_line + \"\\n\")\n",
    "\n",
    "with open(\"data/conll_style/overfit_chinese/\"+ \"train\"+'.chinese.v4_gold_conll', 'w') as f:\n",
    "    f.writelines(document)\n",
    "with open(\"data/conll_style/overfit_chinese/\"+ \"dev\"+'.chinese.v4_gold_conll', 'w') as f:\n",
    "    f.writelines(document)\n",
    "with open(\"data/conll_style/overfit_chinese/\"+ \"test\"+'.chinese.v4_gold_conll', 'w') as f:\n",
    "    f.writelines(document)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44425\n"
     ]
    }
   ],
   "source": [
    "print(len(document))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "file_name = \"test\"\n",
    "data = []\n",
    "with open('data/'+ file_name+'_temp.pkl', 'rb') as f:\n",
    "    data.extend(pkl.load(f))\n",
    "\n",
    "document = []\n",
    "for i in range(len(data)):\n",
    "    if file_name==\"train\" and i==38:\n",
    "        continue\n",
    "    if file_name==\"test\" and i==28:\n",
    "        continue\n",
    "\n",
    "    # if i>=100:\n",
    "    #     continue\n",
    "    sample = data[i]\n",
    "    original_sentences = sample['sentences']\n",
    "    original_clusters = cluster_mentions(sample['answers'], original_sentences)\n",
    "\n",
    "    # Get Data ready for conversion\n",
    "    sentences, clusters, speakers = remove_speaker_prefix(original_sentences, original_clusters)\n",
    "    scene_id = sample['scene_id']\n",
    "    part = int(scene_id[7:9])\n",
    "    begin_line = \"#begin document \" + \"(\" + scene_id + \"); part \" + \"%03d\" % part\n",
    "    end_line = \"#end document\"\n",
    "\n",
    "    # Prepare for clustering\n",
    "    cluster_field = []\n",
    "    for sent in sentences:\n",
    "        cluster_field.append([\"\"]*len(sent))\n",
    "    # Add start\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx)\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx)\n",
    "    # Add start==end\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start == end:\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx) + \")\"\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx) + \")\"\n",
    "    # Add End\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                try:\n",
    "                    if cluster_field[sent_id][end] == \"\":\n",
    "                        cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                    else:\n",
    "                        cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "                except:\n",
    "                    pass\n",
    "                # if cluster_field[sent_id][end] == \"\":\n",
    "                #     cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                # else:\n",
    "                #     cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "\n",
    "    # Build document\n",
    "    document.append(begin_line + \"\\n\")\n",
    "    for sent, speaker, cluster_value in zip(sentences, speakers, cluster_field):\n",
    "        for j, word in enumerate(sent):\n",
    "            cluster_id = cluster_value[j]\n",
    "            if cluster_id == \"\":\n",
    "                cluster_id = \"-\"\n",
    "            temp = [scene_id, str(part), str(j), word, \"na\", \"na\", \"na\", \"na\", \"na\", speaker, \"na\", \"na\", \"na\", cluster_id]\n",
    "            document.append(\" \".join(temp)+ \"\\n\")\n",
    "        document.append(\"\" + \"\\n\")\n",
    "    document.append(end_line + \"\\n\")\n",
    "\n",
    "with open(\"data/input/\"+ file_name+'.english.v4_gold_conll', 'w') as f:\n",
    "    f.writelines(document)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
