{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from copy import deepcopy\n",
    "import jsonlines\n",
    "from utils.my_util import cluster_mentions, remove_speaker_prefix\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check whether data is correct： Correct"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1240 1240\n"
     ]
    }
   ],
   "source": [
    "with open('data/raw_source/dialogue_proj_zh/all_coref_data_en_zh_finalized_word.json', 'r') as f:\n",
    "    original_data = json.load(f)\n",
    "with open('data/raw_source/dialogue_proj_zh/all_coref_data_en_zh_finalized_word_prob.json', 'r') as f:\n",
    "    new_data = json.load(f)\n",
    "print(len(original_data), len(new_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "names = ['sentenceIndex', 'startToken', 'endToken', 'mention_id']\n",
    "for i in range(len(original_data)):\n",
    "    original_sample = original_data[i]\n",
    "    new_sample = new_data[i]\n",
    "    original_annotations = original_sample['annotations']\n",
    "    new_annotations = new_sample['annotations']\n",
    "    for a, b in zip(original_annotations, new_annotations):\n",
    "        if sum([a['query'][name]==b['query'][name] for name in names])!=4:\n",
    "            print(sum([a['query'][name]==b['query'][name] for name in names]))\n",
    "        # print([a['query'][name]==b['query'][name] for name in names], sum([a['query'][name]==b['query'][name] for name in names]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Dialogue Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "speaker_dict = {}\n",
    "with open('data/raw_source/dialogue_zh/all_coref_data_en.json', 'r') as f:\n",
    "    temp = json.load(f)\n",
    "    for line in temp:\n",
    "        scene_id = line['scene_id'][:-1]\n",
    "        speakers = []\n",
    "        for sent in line['sentences']:\n",
    "            speakers.append(\" \".join(sent[:sent.index(\":\")]))\n",
    "        speaker_dict[scene_id] = speakers\n",
    "\n",
    "split_dict = {\"train\":[], \"dev\":[], \"test\":[]}\n",
    "with open('data/raw_source/dialogue_zh/dev_finalized.pkl', 'rb') as f:\n",
    "    temp = pkl.load(f)\n",
    "    for line in temp:\n",
    "        split_dict['dev'].append(line['scene_id'])\n",
    "with open('data/raw_source/dialogue_zh/test_finalized.pkl', 'rb') as f:\n",
    "    temp = pkl.load(f)\n",
    "    for line in temp:\n",
    "        split_dict['test'].append(line['scene_id'])\n",
    "with open('data/raw_source/dialogue_zh/train_finalized.pkl', 'rb') as f:\n",
    "    temp = pkl.load(f)\n",
    "    for line in temp:\n",
    "        split_dict['train'].append(line['scene_id'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def remove_empty_sentences(instance):\n",
    "    sentences = instance['sentences']\n",
    "    answers = instance['answers']\n",
    "    speakers = instance['speakers']\n",
    "\n",
    "    # Build old sent_id to new sent_id map\n",
    "    map_sent_id = {}\n",
    "    count = 0\n",
    "    for i, sent in enumerate(sentences):\n",
    "        if sent == []:\n",
    "            continue\n",
    "        map_sent_id[i] = count\n",
    "        count += 1\n",
    "\n",
    "    # Collect answers, speakers for each sentence\n",
    "    temp = []\n",
    "    for i, sent in enumerate(sentences):\n",
    "        if sent == []:\n",
    "            continue\n",
    "        annotations = []\n",
    "        for answer in answers:\n",
    "            if answer[0][0]==i:\n",
    "                annotations.append(answer)\n",
    "        temp.append([sent, annotations, speakers[i]])\n",
    "\n",
    "    # Change Sentence ID\n",
    "    sentences = []\n",
    "    answers = []\n",
    "    speakers = []\n",
    "    for i, (sent, annotations, speaker) in enumerate(temp):\n",
    "        # print(i, speaker, sent)\n",
    "        sentences.append(sent)\n",
    "        temp_answers = []\n",
    "        for query, antecedents in annotations:\n",
    "            new_query = tuple((map_sent_id[query[0]], query[1], query[2], query[3]))\n",
    "            # print(query, new_query)\n",
    "            new_antecedents = []\n",
    "            if isinstance(antecedents, str):\n",
    "                new_antecedents = antecedents\n",
    "                # print(new_antecedents)\n",
    "            else:\n",
    "                # print(antecedents)\n",
    "                for antecedent in antecedents:\n",
    "                    new_antecedents.append((map_sent_id[antecedent[0]], antecedent[1], antecedent[2], antecedent[3]))\n",
    "            # print(new_antecedents)\n",
    "            temp_answers.append([new_query, new_antecedents])\n",
    "        answers.extend(temp_answers)\n",
    "        speakers.append(speaker)\n",
    "\n",
    "    return {\n",
    "        \"sentences\": sentences,\n",
    "        \"answers\": answers,\n",
    "        \"speakers\": speakers,\n",
    "        \"scene_id\": instance['scene_id']\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cluster Chinese Mentions using English-Side Mention_IDs\n",
    "\"\"\"\n",
    "data = []\n",
    "all_ids = []\n",
    "with open('data/raw_source/dialogue_zh/all_coref_data_en_zh_finalized.json', 'r') as f:\n",
    "# with open('data/raw_source/dialogue_zh/all_coref_data_en_zh_finalized_back.json', 'r') as f:\n",
    "# with open('data/raw_source/dialogue_zh/dev-test-batch1_zh.json', 'r') as f:\n",
    "    reader = jsonlines.Reader(f)\n",
    "    for bulk in reader:\n",
    "        for idx, instance in enumerate(bulk):\n",
    "            # if idx>=10:\n",
    "            #     break\n",
    "            scene_id = instance['scene_id']\n",
    "            if scene_id == \"\":\n",
    "                continue\n",
    "            sentences = instance['sentences']\n",
    "            # print(sentences)\n",
    "            # sentences = [[token for token in \"\".join(sent)] for sent in instance['sentences']]\n",
    "            annotations = instance['annotations']\n",
    "            all_ids.append(scene_id)\n",
    "            speakers = speaker_dict[scene_id]\n",
    "            answers = []\n",
    "            for item in annotations:\n",
    "                # Correct query\n",
    "                query = (item['query']['sentenceIndex'], item['query']['startToken'], item['query']['endToken'], item['query']['mention_id'])\n",
    "                antecedents = item['antecedents']\n",
    "                if antecedents in [['n', 'o', 't', 'P', 'r', 'e', 's', 'e', 'n', 't'], ['null_projection'], ['empty_subtitle']]:\n",
    "                    answers.append([query, \"notPresent\"])\n",
    "                else:\n",
    "                    temp_answer = []\n",
    "                    for antecedent in antecedents:\n",
    "                        if isinstance(antecedent, dict):\n",
    "                            temp_answer.append((antecedent['sentenceIndex'], antecedent['startToken'], antecedent['endToken'], antecedent['mention_id']))\n",
    "                        else:\n",
    "                            temp_answer = \" \".join(antecedents)\n",
    "                    answers.append([query, temp_answer])\n",
    "\n",
    "            # # Add correction results\n",
    "            # if scene_id in correction_result:\n",
    "            #     to_correct = correction_result[scene_id]\n",
    "            #     correction_dict = to_correct['correction_dict']\n",
    "            #     remove_set = to_correct['remove_set']\n",
    "            #     add_dict = to_correct['add_dict']\n",
    "            #     # Deal with to add_dict\n",
    "            #     for item in add_dict:\n",
    "            #         temp = [\n",
    "            #             (add_dict[item][0], add_dict[item][1], add_dict[item][2], item),\n",
    "            #             'notPresent'\n",
    "            #         ]\n",
    "            #         answers.append(temp)\n",
    "\n",
    "            data.append(remove_empty_sentences({\n",
    "                \"sentences\": sentences,\n",
    "                \"answers\": answers,\n",
    "                \"speakers\": speakers,\n",
    "                \"scene_id\": scene_id\n",
    "            }))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentences': [['将', '光子', '正对', '平面', '上', '的', '双缝', '观察', '任意', '一个', '隙缝', '它', '不会', '穿过', '那', '两个', '隙缝', '如果', '没', '被', '观察', '那', '就', '会', '总之', '如果', '观察', '它', '在', '离开', '平面', '到', '击中目标', '之前', '它', '就', '不会', '穿过', '那', '两个', '隙缝'], ['没错', '但', '你', '为什么', '要说', '这个', '?'], ['没什么', '我', '只是', '觉得', '这个', '主意', '可以', '用于', '设计', 'T恤衫'], ['横', '1', '是', 'Aegean', '竖', '8', '是', 'Nabokov', '横', '26', '是', 'MCM', '竖', '14', '是', '...', '手指', '挪开', '点', '这样一来', '横', '14', '就是', 'Port', '瞧', '提示', '是', '\"', 'Papadoc', '的', '首都', '\"', '所以', '是', '太子港', '海地', '的'], ['能', '为', '你', '效劳', '吗', '?'], ['这里', '是', '高智商', '精子', '银行', '吗', '?'], ['如果', '你', '这么', '问', '也许', '你', '不该', '来', '这'], ['我', '想', '就是', '这', '没错', '了'], ['把', '这个', '填一填'], ['谢谢', '我们', '马上', '好'], ['慢慢来', '我', '还要', '玩', '填字游戏', '噢', '慢', '着'], ['我', '办不到'], ['开玩笑', '?', '你', '可是', '半', '职业', '人士'], ['不', '我们', '这样', '是', '诈骗', '我们', '没法', '保证', '生', '出来', '的', '一定', '是', '高智商', '小孩', '我', '姐姐', '跟', '我', '有', '一套', '相同', '的', '基本', '基因', '她', '却', '在', 'Fuddrucker', '餐厅', '当', '服务生'], ['这', '可是', '你', '的', '主意', '啊', '轻松', '赚点', '钱', '就', '有钱', '能', '升级', '我们', '的', '网络带宽'], ['我', '知道', '我', '确实', '很', '渴望', '高速下载', '但', '一些', '可怜', '的', '女人', '会', '把', '希望', '寄托', '在', '我', '的', '精子', '上万', '一生', '出来', '一个', '连', '曲线', '下部', '的', '面积', '用', '积分', '还是', '微分', '算', '都', '搞不清楚', '的', '小屁孩', '怎么办', '?'], ['我', '想', '她', '还是', '会爱', '那个', '宝宝', '的'], ['我', '不会'], ['你', '想要', '怎样', '?'], ['我', '想要', '走'], ['离开', '时要', '怎么', '说', '呢', '?'], ['我', '不', '知道', '我', '可', '从', '没有', '拒绝', '过', '提供', '精子', '的', '要求']], 'answers': [[(0, 1, 2, 's01e01c00t|0'), 'notPresent'], [(0, 3, 4, 's01e01c00t|1'), 'notPresent'], [(0, 6, 11, 's01e01c00t|2'), 'notPresent'], [(0, 8, 9, 's01e01c00t|5'), [(0, 1, 2, 's01e01c00t|6')]], [(0, 11, 12, 's01e01c00t|7'), [(0, 8, 9, 's01e01c00t|8')]], [(0, 15, 16, 's01e01c00t|9'), [(0, 6, 11, 's01e01c00t|10')]], [(0, 21, 22, 's01e01c00t|11'), [(0, 11, 12, 's01e01c00t|12')]], [(0, 27, 28, 's01e01c00t|13'), [(0, 21, 22, 's01e01c00t|14')]], [(0, 30, 31, 's01e01c00t|17'), [(0, 3, 4, 's01e01c00t|18')]], [(0, 32, 33, 's01e01c00t|21'), 'notPresent'], [(0, 34, 35, 's01e01c00t|24'), 'notPresent'], [(0, 39, 41, 's01e01c00t|26'), [(0, 6, 11, 's01e01c00t|27')]], [(0, 3, 11, 's01e01c00t|186'), 'notPresent'], [(1, 3, 4, 's01e01c00t|28'), 'notPresent'], [(1, 2, 6, 's01e01c00t|29'), 'notPresent'], [(1, 2, 3, 's01e01c00t|30'), 'Sheldon'], [(2, 0, 1, 's01e01c00t|33'), [(1, 4, 6, 's01e01c00t|34')]], [(2, 1, 2, 's01e01c00t|35'), 'Sheldon'], [(2, 4, 5, 's01e01c00t|37'), [(0, 1, 41, 's01e01c00t|38')]], [(2, 5, 6, 's01e01c00t|39'), [(2, 4, 5, 's01e01c00t|40')]], [(2, 9, 10, 's01e01c00t|41'), 'notPresent'], [(3, 3, 4, 's01e01c00t|44'), 'notPresent'], [(3, 7, 8, 's01e01c00t|45'), 'notPresent'], [(3, 11, 12, 's01e01c00t|46'), 'notPresent'], [(3, 16, 17, 's01e01c00t|47'), 'Receptionist'], [(3, 18, 19, 's01e01c00t|51'), 'notPresent'], [(3, 23, 24, 's01e01c00t|53'), 'notPresent'], [(3, 25, 31, 's01e01c00t|54'), [(3, 23, 24, 's01e01c00t|55')]], [(3, 32, 33, 's01e01c00t|56'), [(3, 25, 31, 's01e01c00t|57')]], [(3, 34, 36, 's01e01c00t|58'), [(3, 25, 31, 's01e01c00t|59')]], [(3, 34, 35, 's01e01c00t|60'), 'notPresent'], [(4, 2, 3, 's01e01c00t|63'), 'Leonard'], [(5, 0, 1, 's01e01c00t|65'), 'notPresent'], [(5, 2, 5, 's01e01c00t|66'), [(5, 0, 1, 's01e01c00t|67')]], [(6, 1, 2, 's01e01c00t|68'), 'Leonard'], [(6, 5, 6, 's01e01c00t|70'), 'Leonard'], [(7, 0, 1, 's01e01c00t|72'), 'Sheldon'], [(7, 3, 4, 's01e01c00t|74'), [(5, 2, 5, 's01e01c00t|75')]], [(8, 1, 2, 's01e01c00t|78'), 'notPresent'], [(9, 1, 2, 's01e01c00t|81'), 'Sheldon'], [(9, 1, 2, 's01e01c00t|81'), 'Leonard'], [(10, 0, 1, 's01e01c00t|86'), 'notPresent'], [(10, 1, 2, 's01e01c00t|87'), 'Receptionist'], [(10, 4, 5, 's01e01c00t|91'), 'notPresent'], [(11, 0, 1, 's01e01c00t|92'), 'Sheldon'], [(12, 2, 3, 's01e01c00t|98'), 'Sheldon'], [(12, 4, 7, 's01e01c00t|102'), 'notPresent'], [(13, 1, 2, 's01e01c00t|103'), 'Leonard'], [(13, 1, 2, 's01e01c00t|103'), 'Sheldon'], [(13, 4, 5, 's01e01c00t|106'), 'notPresent'], [(13, 15, 16, 's01e01c00t|109'), 'notPresent'], [(13, 15, 16, 's01e01c00t|110'), 'Sheldon'], [(13, 15, 16, 's01e01c00t|110'), 'Leonard'], [(13, 13, 15, 's01e01c00t|113'), 'notPresent'], [(13, 18, 19, 's01e01c00t|116'), 'Sheldon'], [(13, 16, 21, 's01e01c00t|118'), 'notPresent'], [(13, 20, 25, 's01e01c00t|119'), 'notPresent'], [(13, 25, 26, 's01e01c00t|120'), [(13, 16, 17, 's01e01c00t|121')]], [(13, 28, 29, 's01e01c00t|122'), 'notPresent'], [(14, 0, 1, 's01e01c00t|123'), [(13, 4, 5, 's01e01c00t|124')]], [(14, 2, 5, 's01e01c00t|125'), [(14, 0, 1, 's01e01c00t|126')]], [(14, 2, 3, 's01e01c00t|127'), 'Sheldon'], [(14, 7, 11, 's01e01c00t|129'), 'notPresent'], [(14, 12, 16, 's01e01c00t|130'), 'notPresent'], [(14, 14, 15, 's01e01c00t|131'), 'notPresent'], [(15, 0, 1, 's01e01c00t|132'), 'Sheldon'], [(15, 2, 3, 's01e01c00t|134'), 'Sheldon'], [(15, 6, 7, 's01e01c00t|136'), 'notPresent'], [(15, 8, 12, 's01e01c00t|138'), 'notPresent'], [(15, 13, 14, 's01e01c00t|141'), [(15, 9, 12, 's01e01c00t|142')]], [(15, 13, 15, 's01e01c00t|143'), 'notPresent'], [(15, 17, 18, 's01e01c00t|144'), 'Sheldon'], [(15, 17, 20, 's01e01c00t|146'), 'notPresent'], [(15, 37, 38, 's01e01c00t|151'), 'notPresent'], [(15, 36, 37, 's01e01c00t|152'), [(15, 37, 38, 's01e01c00t|153')]], [(15, 30, 31, 's01e01c00t|156'), 'notPresent'], [(15, 28, 29, 's01e01c00t|157'), 'notPresent'], [(15, 25, 26, 's01e01c00t|158'), 'notPresent'], [(16, 0, 1, 's01e01c00t|159'), 'Leonard'], [(16, 2, 3, 's01e01c00t|161'), [(15, 13, 14, 's01e01c00t|162')]], [(16, 5, 6, 's01e01c00t|163'), [(15, 37, 38, 's01e01c00t|164')]], [(17, 0, 1, 's01e01c00t|165'), 'Sheldon'], [(18, 0, 1, 's01e01c00t|168'), 'Sheldon'], [(19, 0, 1, 's01e01c00t|170'), 'Sheldon'], [(20, 1, 4, 's01e01c00t|173'), 'notPresent'], [(21, 0, 1, 's01e01c00t|174'), 'Leonard'], [(21, 3, 4, 's01e01c00t|176'), 'Leonard'], [(21, 9, 13, 's01e01c00t|178'), 'notPresent'], [(21, 10, 11, 's01e01c00t|179'), [(15, 19, 20, 's01e01c00t|180')]]], 'speakers': ['Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Receptionist', 'Leonard', 'Receptionist', 'Sheldon', 'Receptionist', 'Leonard', 'Receptionist', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Sheldon', 'Leonard'], 'scene_id': 's01e01c00t'}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clustering Algorithm with Chinese Correction\n",
    "\"\"\"\n",
    "def cluster_mention_id_index(answers, sentences, en_clusters):\n",
    "    \"\"\"\n",
    "    We cluster Chinese Side mentions according to the index in English Side\n",
    "    \"\"\"\n",
    "    # Collect Mention_ID to Chinese Side tuple\n",
    "    zh_mention_dict = {}\n",
    "    for answer in answers:\n",
    "        query = answer[0]\n",
    "        zh_mention_dict[query[3]] = (query[0], query[1], query[2])\n",
    "        antecedents = answer[1]\n",
    "        if isinstance(antecedents, list):\n",
    "            for antecedent in antecedents:\n",
    "                zh_mention_dict[antecedent[3]] = (antecedent[0], antecedent[1], antecedent[2])\n",
    "\n",
    "    # Incorporate Maunal Correction\n",
    "    scene_id = list(zh_mention_dict.keys())[0]\n",
    "    if scene_id[:10] in correction_result:\n",
    "        to_correct = correction_result[scene_id[:10]]\n",
    "        correction_dict = to_correct['correction_dict']\n",
    "        remove_set = to_correct['remove_set']\n",
    "        source_zh_mention_dict = deepcopy(zh_mention_dict)\n",
    "        zh_mention_dict = {}\n",
    "\n",
    "        # Perform Correction\n",
    "        for mention_id in source_zh_mention_dict:\n",
    "            # remove mentions\n",
    "            if mention_id in remove_set:\n",
    "                continue\n",
    "            # Correct start, end\n",
    "            elif mention_id in correction_dict:\n",
    "                zh_mention_dict[mention_id] = tuple([source_zh_mention_dict[mention_id][0], correction_dict[mention_id][1], correction_dict[mention_id][2]])\n",
    "            else:\n",
    "                zh_mention_dict[mention_id] = source_zh_mention_dict[mention_id]\n",
    "\n",
    "    # Gather Chinese Side cluster according to en_clusters\n",
    "    new_cluster = []\n",
    "    for cluster in en_clusters:\n",
    "        temp = []\n",
    "        for mention_id in cluster:\n",
    "            if mention_id in zh_mention_dict:\n",
    "                temp.append(zh_mention_dict[mention_id])\n",
    "        if temp:\n",
    "            new_cluster.append(temp)\n",
    "\n",
    "    # Merge Cluster using (sent_id, start_id, end_id)\n",
    "    all_clusters = deepcopy(new_cluster)\n",
    "    merged_clusters = []\n",
    "    for cluster in all_clusters:\n",
    "        existing = None\n",
    "        for mention in cluster:\n",
    "            for merged_cluster in merged_clusters:\n",
    "                if mention in merged_cluster:\n",
    "                    existing = merged_cluster\n",
    "                    break\n",
    "            if existing is not None:\n",
    "                break\n",
    "        if existing is not None:\n",
    "            existing.update(cluster)\n",
    "        else:\n",
    "            merged_clusters.append(set(cluster))\n",
    "    merged_clusters = [list(cluster) for cluster in merged_clusters]\n",
    "    return merged_clusters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "\n",
    "with open('en_mention_id_cluster.pkl', 'rb') as f:\n",
    "    en_mention_id_clusters = pkl.load(f)\n",
    "\n",
    "document = []\n",
    "for i in range(len(data)):\n",
    "    # if i > 2:\n",
    "    #     continue\n",
    "    sample = data[i]\n",
    "    if sample['scene_id'] not in split_dict[split]:\n",
    "        continue\n",
    "\n",
    "    # if sample['scene_id'] != \"s05e14c00t0\":\n",
    "    #     continue\n",
    "\n",
    "    # print(sample)\n",
    "    # print()\n",
    "    # original_sentences = sample['sentences']\n",
    "    # original_clusters = cluster_mentions(sample['answers'], original_sentences)\n",
    "    # sentences, clusters, speakers = remove_speaker_prefix(original_sentences, original_clusters)\n",
    "    sentences = sample['sentences']\n",
    "    # clusters = cluster_mentions(sample['answers'], sentences)\n",
    "    speakers = sample['speakers']\n",
    "    scene_id = sample['scene_id']\n",
    "    clusters = cluster_mention_id_index(sample['answers'], sentences, en_mention_id_clusters[scene_id])\n",
    "    part = int(scene_id[7:9])\n",
    "    begin_line = \"#begin document \" + \"(\" + scene_id + \"); part \" + \"%03d\" % part\n",
    "    end_line = \"#end document\"\n",
    "\n",
    "    # Prepare for clustering\n",
    "    cluster_field = []\n",
    "    for sent in sentences:\n",
    "        cluster_field.append([\"\"]*len(sent))\n",
    "    # Add start\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                # print(cluster_field[sent_id])\n",
    "                # print(sent_id, start, end, len(sentences[sent_id]))\n",
    "                # print(sentences[sent_id])\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx)\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx)\n",
    "    # Add start==end\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start == end:\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx) + \")\"\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx) + \")\"\n",
    "    # Add End\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                try:\n",
    "                    if cluster_field[sent_id][end] == \"\":\n",
    "                        cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                    else:\n",
    "                        cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "                except:\n",
    "                    print(\"ERROR\")\n",
    "                    pass\n",
    "                # if cluster_field[sent_id][end] == \"\":\n",
    "                #     cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                # else:\n",
    "                #     cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "\n",
    "    # Build document\n",
    "    document.append(begin_line + \"\\n\")\n",
    "    for sent, speaker, cluster_value in zip(sentences, speakers, cluster_field):\n",
    "        for j, word in enumerate(sent):\n",
    "            cluster_id = cluster_value[j]\n",
    "            if cluster_id == \"\":\n",
    "                cluster_id = \"-\"\n",
    "            temp = [scene_id, str(part), str(j), word, \"na\", \"na\", \"na\", \"na\", \"na\", speaker, \"na\", \"na\", \"na\", cluster_id]\n",
    "            document.append(\" \".join(temp)+ \"\\n\")\n",
    "        document.append(\"\" + \"\\n\")\n",
    "    document.append(end_line + \"\\n\")\n",
    "\n",
    "with open(\"data/conll_style/dialogue_prob_source_chinese/\"+ split+'.chinese.v4_gold_conll', 'w') as f:\n",
    "    f.writelines(document)\n",
    "\n",
    "print(len(document))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}