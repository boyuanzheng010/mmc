{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nIn this script, we want to convert annotations to conll style. There are several steps:\\n1.clustering input data\\n2.Turn cluster data into conll format\\n3.Try to run the preprocess script from HOI code base\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this script, we want to convert annotations to conll style. There are several steps:\n",
    "1.clustering input data\n",
    "2.Turn cluster data into conll format\n",
    "3.Try to run the preprocess script from HOI code base\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from copy import deepcopy\n",
    "import jsonlines\n",
    "from utils.my_util import cluster_mentions, remove_speaker_prefix\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Dialogue Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "speaker_dict = {}\n",
    "with open('data/raw_source/dialogue_zh/all_coref_data_en.json', 'r') as f:\n",
    "    temp = json.load(f)\n",
    "    for line in temp:\n",
    "        scene_id = line['scene_id'][:-1]\n",
    "        speakers = []\n",
    "        for sent in line['sentences']:\n",
    "            speakers.append(\" \".join(sent[:sent.index(\":\")]))\n",
    "        speaker_dict[scene_id] = speakers\n",
    "\n",
    "split_dict = {\"train\":[], \"dev\":[], \"test\":[]}\n",
    "with open('data/raw_source/dialogue_zh/dev_finalized.pkl', 'rb') as f:\n",
    "    temp = pkl.load(f)\n",
    "    for line in temp:\n",
    "        split_dict['dev'].append(line['scene_id'])\n",
    "with open('data/raw_source/dialogue_zh/test_finalized.pkl', 'rb') as f:\n",
    "    temp = pkl.load(f)\n",
    "    for line in temp:\n",
    "        split_dict['test'].append(line['scene_id'])\n",
    "with open('data/raw_source/dialogue_zh/train_finalized.pkl', 'rb') as f:\n",
    "    temp = pkl.load(f)\n",
    "    for line in temp:\n",
    "        split_dict['train'].append(line['scene_id'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "with open(\"data/raw_source/dialogue_zh/all_correction_results.pkl\", 'rb') as f:\n",
    "    correction_result = pkl.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def remove_empty_sentences(instance):\n",
    "    sentences = instance['sentences']\n",
    "    answers = instance['answers']\n",
    "    speakers = instance['speakers']\n",
    "\n",
    "    # Build old sent_id to new sent_id map\n",
    "    map_sent_id = {}\n",
    "    count = 0\n",
    "    for i, sent in enumerate(sentences):\n",
    "        if sent == []:\n",
    "            continue\n",
    "        map_sent_id[i] = count\n",
    "        count += 1\n",
    "\n",
    "    # Collect answers, speakers for each sentence\n",
    "    temp = []\n",
    "    for i, sent in enumerate(sentences):\n",
    "        if sent == []:\n",
    "            continue\n",
    "        annotations = []\n",
    "        for answer in answers:\n",
    "            if answer[0][0]==i:\n",
    "                annotations.append(answer)\n",
    "        temp.append([sent, annotations, speakers[i]])\n",
    "\n",
    "    # Change Sentence ID\n",
    "    sentences = []\n",
    "    answers = []\n",
    "    speakers = []\n",
    "    for i, (sent, annotations, speaker) in enumerate(temp):\n",
    "        # print(i, speaker, sent)\n",
    "        sentences.append(sent)\n",
    "        temp_answers = []\n",
    "        for query, antecedents in annotations:\n",
    "            new_query = tuple((map_sent_id[query[0]], query[1], query[2], query[3]))\n",
    "            # print(query, new_query)\n",
    "            new_antecedents = []\n",
    "            if isinstance(antecedents, str):\n",
    "                new_antecedents = antecedents\n",
    "                # print(new_antecedents)\n",
    "            else:\n",
    "                # print(antecedents)\n",
    "                for antecedent in antecedents:\n",
    "                    new_antecedents.append((map_sent_id[antecedent[0]], antecedent[1], antecedent[2], antecedent[3]))\n",
    "            # print(new_antecedents)\n",
    "            temp_answers.append([new_query, new_antecedents])\n",
    "        answers.extend(temp_answers)\n",
    "        speakers.append(speaker)\n",
    "\n",
    "    return {\n",
    "        \"sentences\": sentences,\n",
    "        \"answers\": answers,\n",
    "        \"speakers\": speakers,\n",
    "        \"scene_id\": instance['scene_id']\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cluster Chinese Mentions using English-Side Mention_IDs\n",
    "\"\"\"\n",
    "data = []\n",
    "all_ids = []\n",
    "with open('data/raw_source/dialogue_zh/all_coref_data_en_zh_finalized.json', 'r') as f:\n",
    "# with open('data/raw_source/dialogue_zh/all_coref_data_en_zh_finalized_back.json', 'r') as f:\n",
    "# with open('data/raw_source/dialogue_zh/dev-test-batch1_zh.json', 'r') as f:\n",
    "    reader = jsonlines.Reader(f)\n",
    "    for bulk in reader:\n",
    "        for idx, instance in enumerate(bulk):\n",
    "            # if idx>=10:\n",
    "            #     break\n",
    "            scene_id = instance['scene_id']\n",
    "            if scene_id == \"\":\n",
    "                continue\n",
    "            sentences = instance['sentences']\n",
    "            # print(sentences)\n",
    "            # sentences = [[token for token in \"\".join(sent)] for sent in instance['sentences']]\n",
    "            annotations = instance['annotations']\n",
    "            all_ids.append(scene_id)\n",
    "            speakers = speaker_dict[scene_id]\n",
    "            answers = []\n",
    "            for item in annotations:\n",
    "                # Correct query\n",
    "                query = (item['query']['sentenceIndex'], item['query']['startToken'], item['query']['endToken'], item['query']['mention_id'])\n",
    "                antecedents = item['antecedents']\n",
    "                if antecedents in [['n', 'o', 't', 'P', 'r', 'e', 's', 'e', 'n', 't'], ['null_projection'], ['empty_subtitle']]:\n",
    "                    answers.append([query, \"notPresent\"])\n",
    "                else:\n",
    "                    temp_answer = []\n",
    "                    for antecedent in antecedents:\n",
    "                        if isinstance(antecedent, dict):\n",
    "                            temp_answer.append((antecedent['sentenceIndex'], antecedent['startToken'], antecedent['endToken'], antecedent['mention_id']))\n",
    "                        else:\n",
    "                            temp_answer = \" \".join(antecedents)\n",
    "                    answers.append([query, temp_answer])\n",
    "\n",
    "            # Add correction results\n",
    "            if scene_id in correction_result:\n",
    "                to_correct = correction_result[scene_id]\n",
    "                correction_dict = to_correct['correction_dict']\n",
    "                remove_set = to_correct['remove_set']\n",
    "                add_dict = to_correct['add_dict']\n",
    "                # Deal with to add_dict\n",
    "                for item in add_dict:\n",
    "                    temp = [\n",
    "                        (add_dict[item][0], add_dict[item][1], add_dict[item][2], item),\n",
    "                        'notPresent'\n",
    "                    ]\n",
    "                    answers.append(temp)\n",
    "\n",
    "            data.append(remove_empty_sentences({\n",
    "                \"sentences\": sentences,\n",
    "                \"answers\": answers,\n",
    "                \"speakers\": speakers,\n",
    "                \"scene_id\": scene_id\n",
    "            }))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1231\n",
      "1222\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(955+134+133)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# with open('en_mention_id_cluster.pkl', 'rb') as f:\n",
    "#     en_mention_id_clusters = pkl.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clustering Algorithm\n",
    "\"\"\"\n",
    "def cluster_mention_id_index(answers, sentences, en_clusters):\n",
    "    \"\"\"\n",
    "    We cluster Chinese Side mentions according to the index in English Side\n",
    "    \"\"\"\n",
    "    # Collect Mention_ID to Chinese Side tuple\n",
    "    zh_mention_dict = {}\n",
    "    for answer in answers:\n",
    "        query = answer[0]\n",
    "        zh_mention_dict[query[3]] = (query[0], query[1], query[2])\n",
    "        antecedents = answer[1]\n",
    "        if isinstance(antecedents, list):\n",
    "            for antecedent in antecedents:\n",
    "                zh_mention_dict[antecedent[3]] = (antecedent[0], antecedent[1], antecedent[2])\n",
    "    # Gather Chinese Side cluster according to en_clusters\n",
    "    new_cluster = []\n",
    "    for cluster in en_clusters:\n",
    "        temp = []\n",
    "        for mention_id in cluster:\n",
    "            if mention_id in zh_mention_dict:\n",
    "                temp.append(zh_mention_dict[mention_id])\n",
    "        if temp:\n",
    "            new_cluster.append(temp)\n",
    "\n",
    "    # Merge Cluster using (sent_id, start_id, end_id)\n",
    "    all_clusters = deepcopy(new_cluster)\n",
    "    merged_clusters = []\n",
    "    for cluster in all_clusters:\n",
    "        existing = None\n",
    "        for mention in cluster:\n",
    "            for merged_cluster in merged_clusters:\n",
    "                if mention in merged_cluster:\n",
    "                    existing = merged_cluster\n",
    "                    break\n",
    "            if existing is not None:\n",
    "                break\n",
    "        if existing is not None:\n",
    "            existing.update(cluster)\n",
    "        else:\n",
    "            merged_clusters.append(set(cluster))\n",
    "    merged_clusters = [list(cluster) for cluster in merged_clusters]\n",
    "    return merged_clusters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# with open(\"data/raw_source/dialogue_zh/correction_results.pkl\", 'rb') as f:\n",
    "#     correction_result = pkl.load(f)\n",
    "\"\"\"\n",
    "Clustering Algorithm with Chinese Correction\n",
    "\"\"\"\n",
    "def cluster_mention_id_index(answers, sentences, en_clusters):\n",
    "    \"\"\"\n",
    "    We cluster Chinese Side mentions according to the index in English Side\n",
    "    \"\"\"\n",
    "    # Collect Mention_ID to Chinese Side tuple\n",
    "    zh_mention_dict = {}\n",
    "    for answer in answers:\n",
    "        query = answer[0]\n",
    "        zh_mention_dict[query[3]] = (query[0], query[1], query[2])\n",
    "        antecedents = answer[1]\n",
    "        if isinstance(antecedents, list):\n",
    "            for antecedent in antecedents:\n",
    "                zh_mention_dict[antecedent[3]] = (antecedent[0], antecedent[1], antecedent[2])\n",
    "\n",
    "    # Incorporate Maunal Correction\n",
    "    scene_id = list(zh_mention_dict.keys())[0]\n",
    "    if scene_id[:10] in correction_result:\n",
    "        to_correct = correction_result[scene_id[:10]]\n",
    "        correction_dict = to_correct['correction_dict']\n",
    "        remove_set = to_correct['remove_set']\n",
    "        source_zh_mention_dict = deepcopy(zh_mention_dict)\n",
    "        zh_mention_dict = {}\n",
    "\n",
    "        # Perform Correction\n",
    "        for mention_id in source_zh_mention_dict:\n",
    "            # remove mentions\n",
    "            if mention_id in remove_set:\n",
    "                continue\n",
    "            # Correct start, end\n",
    "            elif mention_id in correction_dict:\n",
    "                zh_mention_dict[mention_id] = tuple([source_zh_mention_dict[mention_id][0], correction_dict[mention_id][1], correction_dict[mention_id][2]])\n",
    "            else:\n",
    "                zh_mention_dict[mention_id] = source_zh_mention_dict[mention_id]\n",
    "\n",
    "    # Gather Chinese Side cluster according to en_clusters\n",
    "    new_cluster = []\n",
    "    for cluster in en_clusters:\n",
    "        temp = []\n",
    "        for mention_id in cluster:\n",
    "            if mention_id in zh_mention_dict:\n",
    "                temp.append(zh_mention_dict[mention_id])\n",
    "        if temp:\n",
    "            new_cluster.append(temp)\n",
    "\n",
    "    # Merge Cluster using (sent_id, start_id, end_id)\n",
    "    all_clusters = deepcopy(new_cluster)\n",
    "    merged_clusters = []\n",
    "    for cluster in all_clusters:\n",
    "        existing = None\n",
    "        for mention in cluster:\n",
    "            for merged_cluster in merged_clusters:\n",
    "                if mention in merged_cluster:\n",
    "                    existing = merged_cluster\n",
    "                    break\n",
    "            if existing is not None:\n",
    "                break\n",
    "        if existing is not None:\n",
    "            existing.update(cluster)\n",
    "        else:\n",
    "            merged_clusters.append(set(cluster))\n",
    "    merged_clusters = [list(cluster) for cluster in merged_clusters]\n",
    "    return merged_clusters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195730\n"
     ]
    }
   ],
   "source": [
    "split = \"train\"\n",
    "\n",
    "with open('en_mention_id_cluster.pkl', 'rb') as f:\n",
    "    en_mention_id_clusters = pkl.load(f)\n",
    "\n",
    "document = []\n",
    "for i in range(len(data)):\n",
    "    # if i > 2:\n",
    "    #     continue\n",
    "    sample = data[i]\n",
    "    if sample['scene_id'] not in split_dict[split]:\n",
    "        continue\n",
    "\n",
    "    # if sample['scene_id'] != \"s05e14c00t0\":\n",
    "    #     continue\n",
    "\n",
    "    # print(sample)\n",
    "    # print()\n",
    "    # original_sentences = sample['sentences']\n",
    "    # original_clusters = cluster_mentions(sample['answers'], original_sentences)\n",
    "    # sentences, clusters, speakers = remove_speaker_prefix(original_sentences, original_clusters)\n",
    "    sentences = sample['sentences']\n",
    "    # clusters = cluster_mentions(sample['answers'], sentences)\n",
    "    speakers = sample['speakers']\n",
    "    scene_id = sample['scene_id']\n",
    "    clusters = cluster_mention_id_index(sample['answers'], sentences, en_mention_id_clusters[scene_id])\n",
    "    part = int(scene_id[7:9])\n",
    "    begin_line = \"#begin document \" + \"(\" + scene_id + \"); part \" + \"%03d\" % part\n",
    "    end_line = \"#end document\"\n",
    "\n",
    "    # Prepare for clustering\n",
    "    cluster_field = []\n",
    "    for sent in sentences:\n",
    "        cluster_field.append([\"\"]*len(sent))\n",
    "    # Add start\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                # print(cluster_field[sent_id])\n",
    "                # print(sent_id, start, end, len(sentences[sent_id]))\n",
    "                # print(sentences[sent_id])\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx)\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx)\n",
    "    # Add start==end\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start == end:\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx) + \")\"\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx) + \")\"\n",
    "    # Add End\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                try:\n",
    "                    if cluster_field[sent_id][end] == \"\":\n",
    "                        cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                    else:\n",
    "                        cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "                except:\n",
    "                    print(\"ERROR\")\n",
    "                    pass\n",
    "                # if cluster_field[sent_id][end] == \"\":\n",
    "                #     cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                # else:\n",
    "                #     cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "\n",
    "    # Build document\n",
    "    document.append(begin_line + \"\\n\")\n",
    "    for sent, speaker, cluster_value in zip(sentences, speakers, cluster_field):\n",
    "        for j, word in enumerate(sent):\n",
    "            cluster_id = cluster_value[j]\n",
    "            if cluster_id == \"\":\n",
    "                cluster_id = \"-\"\n",
    "            temp = [scene_id, str(part), str(j), word, \"na\", \"na\", \"na\", \"na\", \"na\", speaker, \"na\", \"na\", \"na\", cluster_id]\n",
    "            document.append(\" \".join(temp)+ \"\\n\")\n",
    "        document.append(\"\" + \"\\n\")\n",
    "    document.append(end_line + \"\\n\")\n",
    "\n",
    "with open(\"data/conll_style/chinese_temp/\"+ split+'.chinese.v4_gold_conll', 'w') as f:\n",
    "    f.writelines(document)\n",
    "\n",
    "print(len(document))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Chinese Sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1215\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "\n",
    "document = []\n",
    "for i in range(len(data[:1])):\n",
    "    sample = data[i]\n",
    "    if sample['scene_id'] not in split_dict[split]:\n",
    "        continue\n",
    "    # print(sample)\n",
    "    # print()\n",
    "    # original_sentences = sample['sentences']\n",
    "    # original_clusters = cluster_mentions(sample['answers'], original_sentences)\n",
    "    # sentences, clusters, speakers = remove_speaker_prefix(original_sentences, original_clusters)\n",
    "    sentences = sample['sentences']\n",
    "    clusters = cluster_mentions(sample['answers'], sentences)\n",
    "    speakers = sample['speakers']\n",
    "    scene_id = sample['scene_id']\n",
    "    part = int(scene_id[7:9])\n",
    "    begin_line = \"#begin document \" + \"(\" + scene_id + \"); part \" + \"%03d\" % part\n",
    "    end_line = \"#end document\"\n",
    "\n",
    "    # Prepare for clustering\n",
    "    cluster_field = []\n",
    "    for sent in sentences:\n",
    "        cluster_field.append([\"\"]*len(sent))\n",
    "    # Add start\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                # print(cluster_field[sent_id])\n",
    "                # print(sent_id, start, end, len(sentences[sent_id]))\n",
    "                # print(sentences[sent_id])\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx)\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx)\n",
    "    # Add start==end\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start == end:\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx) + \")\"\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx) + \")\"\n",
    "    # Add End\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                try:\n",
    "                    if cluster_field[sent_id][end] == \"\":\n",
    "                        cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                    else:\n",
    "                        cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "                except:\n",
    "                    pass\n",
    "                # if cluster_field[sent_id][end] == \"\":\n",
    "                #     cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                # else:\n",
    "                #     cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "\n",
    "    # Build document\n",
    "    document.append(begin_line + \"\\n\")\n",
    "    for sent, speaker, cluster_value in zip(sentences, speakers, cluster_field):\n",
    "        for j, word in enumerate(sent):\n",
    "            cluster_id = cluster_value[j]\n",
    "            if cluster_id == \"\":\n",
    "                cluster_id = \"-\"\n",
    "            temp = [scene_id, str(part), str(j), word, \"na\", \"na\", \"na\", \"na\", \"na\", speaker, \"na\", \"na\", \"na\", cluster_id]\n",
    "            document.append(\" \".join(temp)+ \"\\n\")\n",
    "        document.append(\"\" + \"\\n\")\n",
    "    document.append(end_line + \"\\n\")\n",
    "\n",
    "with open(\"data/conll_style/overfit_chinese/\"+ \"train\"+'.chinese.v4_gold_conll', 'w') as f:\n",
    "    f.writelines(document)\n",
    "with open(\"data/conll_style/overfit_chinese/\"+ \"dev\"+'.chinese.v4_gold_conll', 'w') as f:\n",
    "    f.writelines(document)\n",
    "with open(\"data/conll_style/overfit_chinese/\"+ \"test\"+'.chinese.v4_gold_conll', 'w') as f:\n",
    "    f.writelines(document)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44425\n"
     ]
    }
   ],
   "source": [
    "print(len(document))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "file_name = \"test\"\n",
    "data = []\n",
    "with open('data/'+ file_name+'_temp.pkl', 'rb') as f:\n",
    "    data.extend(pkl.load(f))\n",
    "\n",
    "document = []\n",
    "for i in range(len(data)):\n",
    "    if file_name==\"train\" and i==38:\n",
    "        continue\n",
    "    if file_name==\"test\" and i==28:\n",
    "        continue\n",
    "\n",
    "    # if i>=100:\n",
    "    #     continue\n",
    "    sample = data[i]\n",
    "    original_sentences = sample['sentences']\n",
    "    original_clusters = cluster_mentions(sample['answers'], original_sentences)\n",
    "\n",
    "    # Get Data ready for conversion\n",
    "    sentences, clusters, speakers = remove_speaker_prefix(original_sentences, original_clusters)\n",
    "    scene_id = sample['scene_id']\n",
    "    part = int(scene_id[7:9])\n",
    "    begin_line = \"#begin document \" + \"(\" + scene_id + \"); part \" + \"%03d\" % part\n",
    "    end_line = \"#end document\"\n",
    "\n",
    "    # Prepare for clustering\n",
    "    cluster_field = []\n",
    "    for sent in sentences:\n",
    "        cluster_field.append([\"\"]*len(sent))\n",
    "    # Add start\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx)\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx)\n",
    "    # Add start==end\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start == end:\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx) + \")\"\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx) + \")\"\n",
    "    # Add End\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                try:\n",
    "                    if cluster_field[sent_id][end] == \"\":\n",
    "                        cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                    else:\n",
    "                        cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "                except:\n",
    "                    pass\n",
    "                # if cluster_field[sent_id][end] == \"\":\n",
    "                #     cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                # else:\n",
    "                #     cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "\n",
    "    # Build document\n",
    "    document.append(begin_line + \"\\n\")\n",
    "    for sent, speaker, cluster_value in zip(sentences, speakers, cluster_field):\n",
    "        for j, word in enumerate(sent):\n",
    "            cluster_id = cluster_value[j]\n",
    "            if cluster_id == \"\":\n",
    "                cluster_id = \"-\"\n",
    "            temp = [scene_id, str(part), str(j), word, \"na\", \"na\", \"na\", \"na\", \"na\", speaker, \"na\", \"na\", \"na\", cluster_id]\n",
    "            document.append(\" \".join(temp)+ \"\\n\")\n",
    "        document.append(\"\" + \"\\n\")\n",
    "    document.append(end_line + \"\\n\")\n",
    "\n",
    "with open(\"data/input/\"+ file_name+'.english.v4_gold_conll', 'w') as f:\n",
    "    f.writelines(document)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}