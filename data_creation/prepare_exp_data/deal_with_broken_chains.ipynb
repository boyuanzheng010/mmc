{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nIn this script, we want to convert annotations to conll style. There are several steps:\\n1.clustering input data\\n2.Turn cluster data into conll format\\n3.Try to run the preprocess script from HOI code base\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this script, we want to convert annotations to conll style. There are several steps:\n",
    "1.clustering input data\n",
    "2.Turn cluster data into conll format\n",
    "3.Try to run the preprocess script from HOI code base\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from copy import deepcopy\n",
    "import jsonlines\n",
    "from utils.my_util import cluster_mentions, remove_speaker_prefix\n",
    "import json\n",
    "from collections import defaultdict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Dialogue Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "speaker_dict = {}\n",
    "with open('data/raw_source/dialogue_zh/all_coref_data_en.json', 'r') as f:\n",
    "    temp = json.load(f)\n",
    "    for line in temp:\n",
    "        scene_id = line['scene_id'][:-1]\n",
    "        speakers = []\n",
    "        for sent in line['sentences']:\n",
    "            speakers.append(\" \".join(sent[:sent.index(\":\")]))\n",
    "        speaker_dict[scene_id] = speakers\n",
    "\n",
    "split_dict = {\"train\":[], \"dev\":[], \"test\":[]}\n",
    "with open('data/raw_source/dialogue_zh/dev_finalized.pkl', 'rb') as f:\n",
    "    temp = pkl.load(f)\n",
    "    for line in temp:\n",
    "        split_dict['dev'].append(line['scene_id'])\n",
    "with open('data/raw_source/dialogue_zh/test_finalized.pkl', 'rb') as f:\n",
    "    temp = pkl.load(f)\n",
    "    for line in temp:\n",
    "        split_dict['test'].append(line['scene_id'])\n",
    "with open('data/raw_source/dialogue_zh/train_finalized.pkl', 'rb') as f:\n",
    "    temp = pkl.load(f)\n",
    "    for line in temp:\n",
    "        split_dict['train'].append(line['scene_id'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def remove_empty_sentences(instance):\n",
    "    sentences = instance['sentences']\n",
    "    answers = instance['answers']\n",
    "    speakers = instance['speakers']\n",
    "\n",
    "    # Build old sent_id to new sent_id map\n",
    "    map_sent_id = {}\n",
    "    count = 0\n",
    "    for i, sent in enumerate(sentences):\n",
    "        if sent == []:\n",
    "            continue\n",
    "        map_sent_id[i] = count\n",
    "        count += 1\n",
    "\n",
    "    # Collect answers, speakers for each sentence\n",
    "    temp = []\n",
    "    for i, sent in enumerate(sentences):\n",
    "        if sent == []:\n",
    "            continue\n",
    "        annotations = []\n",
    "        for answer in answers:\n",
    "            if answer[0][0]==i:\n",
    "                annotations.append(answer)\n",
    "        temp.append([sent, annotations, speakers[i]])\n",
    "\n",
    "    # Change Sentence ID\n",
    "    sentences = []\n",
    "    answers = []\n",
    "    speakers = []\n",
    "    for i, (sent, annotations, speaker) in enumerate(temp):\n",
    "        # print(i, speaker, sent)\n",
    "        sentences.append(sent)\n",
    "        temp_answers = []\n",
    "        for query, antecedents in annotations:\n",
    "            new_query = tuple((map_sent_id[query[0]], query[1], query[2]))\n",
    "            # print(query, new_query)\n",
    "            new_antecedents = []\n",
    "            if isinstance(antecedents, str):\n",
    "                new_antecedents = antecedents\n",
    "                # print(new_antecedents)\n",
    "            else:\n",
    "                # print(antecedents)\n",
    "                for antecedent in antecedents:\n",
    "                    new_antecedents.append((map_sent_id[antecedent[0]], antecedent[1], antecedent[2]))\n",
    "            # print(new_antecedents)\n",
    "            temp_answers.append([new_query, new_antecedents])\n",
    "        answers.extend(temp_answers)\n",
    "        speakers.append(speaker)\n",
    "\n",
    "    return {\n",
    "        \"sentences\": sentences,\n",
    "        \"answers\": answers,\n",
    "        \"speakers\": speakers,\n",
    "        \"scene_id\": instance['scene_id']\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def cluster_en_mentions_with_id(answers, sentences):\n",
    "    \"\"\"\n",
    "    Cluster mention including plural. The clustering steps are as follows:\n",
    "    1. Gather all non-plural query-annotation pairs\n",
    "    2. Merge all non-plural pairs to build big cluster\n",
    "    3. Add Plural: turn each mention in plural into {1.speaker name, 2.cluster id, 3.turn sent_id, start, end into special identification}, then use these to build a string for clustering\n",
    "    4. Add each pair to cluster and do merging\n",
    "    5. Remove strings from each cluster\n",
    "    ?One Thing to Consider: Whether use speaker mention in sentence to merge to speaker cluster?\n",
    "    \"\"\"\n",
    "    all_clusters = []\n",
    "    speaker_set = set()\n",
    "    # Generate all cluster (no plural), we will add plural latter\n",
    "    for query, annotations in answers:\n",
    "        if isinstance(annotations, str):\n",
    "            if annotations==\"notPresent\":\n",
    "                all_clusters.append([query])\n",
    "        elif len(annotations)==1:\n",
    "            temp = [query]\n",
    "            for token in annotations:\n",
    "                if token[1]==0:\n",
    "                    try:\n",
    "                        speaker = \" \".join(sentences[token[0]][token[1]: sentences[token[0]].index(\":\")]).lower()\n",
    "                        temp.append(speaker)\n",
    "                        speaker_set.add(speaker)\n",
    "                    except:\n",
    "                        continue\n",
    "                else:\n",
    "                    temp.append(token)\n",
    "            all_clusters.append(temp)\n",
    "\n",
    "    # Merge clusters if any clusters have common mentions\n",
    "    merged_clusters = []\n",
    "    for cluster in all_clusters:\n",
    "        existing = None\n",
    "        for mention in cluster:\n",
    "            for merged_cluster in merged_clusters:\n",
    "                if mention in merged_cluster:\n",
    "                    existing = merged_cluster\n",
    "                    break\n",
    "            if existing is not None:\n",
    "                break\n",
    "        if existing is not None:\n",
    "            existing.update(cluster)\n",
    "        else:\n",
    "            merged_clusters.append(set(cluster))\n",
    "    merged_clusters = [list(cluster) for cluster in merged_clusters]\n",
    "\n",
    "    # Add Plural\n",
    "    for query, annotations in answers:\n",
    "        if isinstance(annotations, list):\n",
    "            if len(annotations)>1:\n",
    "                temp_anno = []\n",
    "                for token in annotations:\n",
    "                    if token[1]==0:\n",
    "                        speaker = \" \".join(sentences[token[0]][token[1]: sentences[token[0]].index(\":\")]).lower()\n",
    "                        temp_anno.append(speaker)\n",
    "                        speaker_set.add(speaker)\n",
    "                    # if token[1]==0:\n",
    "                    #     try:\n",
    "                    #         speaker = \" \".join(sentences[token[0]][token[1]: sentences[token[0]].index(\":\")]).lower()\n",
    "                    #         temp_anno.append(speaker)\n",
    "                    #         speaker_set.add(speaker)\n",
    "                    #     except:\n",
    "                    #         continue\n",
    "                    else:\n",
    "                        # If the cluster is already in cluster, use the cluster id as identification, else use the index\n",
    "                        cluster_idx = -1\n",
    "                        for idx, cluster in enumerate(merged_clusters):\n",
    "                            if token in cluster:\n",
    "                                cluster_idx = idx\n",
    "                                break\n",
    "                        if cluster_idx != -1:\n",
    "                            temp_anno.append(str(cluster_idx))\n",
    "                        else:\n",
    "                            temp_anno.append(\"*\" + \"*\".join([str(num) for num in token]) + \"*\")\n",
    "                temp_cluster = [query, \"||\".join(sorted(temp_anno))]\n",
    "                merged_clusters.append(temp_cluster)\n",
    "\n",
    "    # Merge Plural\n",
    "    all_clusters = deepcopy(merged_clusters)\n",
    "    merged_clusters = []\n",
    "    for cluster in all_clusters:\n",
    "        existing = None\n",
    "        for mention in cluster:\n",
    "            for merged_cluster in merged_clusters:\n",
    "                if mention in merged_cluster:\n",
    "                    existing = merged_cluster\n",
    "                    break\n",
    "            if existing is not None:\n",
    "                break\n",
    "        if existing is not None:\n",
    "            existing.update(cluster)\n",
    "        else:\n",
    "            merged_clusters.append(set(cluster))\n",
    "    merged_clusters = [list(cluster) for cluster in merged_clusters]\n",
    "\n",
    "    temp_output = []\n",
    "    for cluster in merged_clusters:\n",
    "        temp_output.append([token for token in cluster if isinstance(token, tuple)])\n",
    "    return temp_output\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build Mention ID Clusters in English Side\n",
    "\"\"\"\n",
    "\n",
    "en_data = []\n",
    "en_all_ids = []\n",
    "en_mention_id_clusters = {}\n",
    "with open('data/raw_source/dialogue_en/all_coref_data_en_finalized.json', 'r') as f:\n",
    "# with open('data/raw_source/dialogue_zh/all_coref_data_en_zh_seg.json', 'r') as f:\n",
    "# with open('data/raw_source/dialogue_zh/dev-test-batch1_zh.json', 'r') as f:\n",
    "    reader = jsonlines.Reader(f)\n",
    "    for bulk in reader:\n",
    "        for idx, instance in enumerate(bulk):\n",
    "            # if idx>=5:\n",
    "            #     break\n",
    "            scene_id = instance['scene_id']\n",
    "            if scene_id == \"\":\n",
    "                continue\n",
    "            sentences = instance['sentences']\n",
    "            # print(sentences)\n",
    "            # sentences = [[token for token in \"\".join(sent)] for sent in instance['sentences']]\n",
    "            annotations = instance['annotations']\n",
    "\n",
    "            # Build tuple_id to mention_id dictionary\n",
    "            tuple_mention_dict = {}\n",
    "            for item in annotations:\n",
    "                query = tuple((item['query']['sentenceIndex'], item['query']['startToken'], item['query']['endToken']))\n",
    "                antecedents = item['antecedents']\n",
    "                for antecedent in antecedents:\n",
    "                    if isinstance(antecedent, dict):\n",
    "                        temp_antecedent = tuple((antecedent['sentenceIndex'], antecedent['startToken'], antecedent['endToken']))\n",
    "                        tuple_mention_dict[temp_antecedent] = antecedent['mention_id']\n",
    "                tuple_mention_dict[query] = item['query']['mention_id']\n",
    "            # print(tuple_mention_dict)\n",
    "            # print()\n",
    "\n",
    "            en_all_ids.append(scene_id)\n",
    "            speakers = speaker_dict[scene_id]\n",
    "            answers = []\n",
    "            for item in annotations:\n",
    "                query = (item['query']['sentenceIndex'], item['query']['startToken'], item['query']['endToken'])\n",
    "                antecedents = item['antecedents']\n",
    "                # print(query)\n",
    "                # print(antecedents)\n",
    "                # print()\n",
    "                if antecedents in [['n', 'o', 't', 'P', 'r', 'e', 's', 'e', 'n', 't'], ['null_projection'], ['empty_subtitle']]:\n",
    "                    answers.append([query, \"notPresent\"])\n",
    "                else:\n",
    "                    temp_answer = []\n",
    "                    for antecedent in antecedents:\n",
    "                        if isinstance(antecedent, dict):\n",
    "                            temp_answer.append((antecedent['sentenceIndex'], antecedent['startToken'], antecedent['endToken']))\n",
    "                        else:\n",
    "                            temp_answer = \" \".join(antecedents)\n",
    "                    answers.append([query, temp_answer])\n",
    "            mention_id_cluster = []\n",
    "            for cluster in cluster_en_mentions_with_id(answers, sentences):\n",
    "                temp = []\n",
    "                for mention in cluster:\n",
    "                    temp.append(tuple_mention_dict[mention])\n",
    "                mention_id_cluster.append(temp)\n",
    "            en_mention_id_clusters[scene_id] = mention_id_cluster\n",
    "\n",
    "            # print(mention_id_cluster)\n",
    "            # print(\"==\"*50)\n",
    "\n",
    "            en_data.append({\n",
    "                \"sentences\": sentences,\n",
    "                \"answers\": answers,\n",
    "                \"speakers\": speakers,\n",
    "                \"scene_id\": scene_id,\n",
    "                \"mention_id_cluster\": mention_id_cluster\n",
    "            })\n",
    "\n",
    "with open('en_mention_id_cluster.pkl', 'wb') as f:\n",
    "    pkl.dump(en_mention_id_clusters, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load English Mention ID Cluster\n",
    "\"\"\"\n",
    "with open('en_mention_id_cluster.pkl', 'rb') as f:\n",
    "    en_mention_id_clusters = pkl.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get Projected Data in Chinese Side\n",
    "\"\"\"\n",
    "\n",
    "zh_data = []\n",
    "zh_all_ids = []\n",
    "with open('data/raw_source/dialogue_zh/all_coref_data_en_zh_with_id.json', 'r') as f:\n",
    "# with open('data/raw_source/dialogue_zh/all_coref_data_en_zh_seg.json', 'r') as f:\n",
    "# with open('data/raw_source/dialogue_zh/dev-test-batch1_zh.json', 'r') as f:\n",
    "    reader = jsonlines.Reader(f)\n",
    "    for bulk in reader:\n",
    "        for idx, instance in enumerate(bulk):\n",
    "            if idx>=5:\n",
    "                break\n",
    "            scene_id = instance['scene_id']\n",
    "            if scene_id == \"\":\n",
    "                continue\n",
    "            sentences = instance['sentences']\n",
    "            # print(sentences)\n",
    "            # sentences = [[token for token in \"\".join(sent)] for sent in instance['sentences']]\n",
    "            annotations = instance['annotations']\n",
    "\n",
    "            zh_all_ids.append(scene_id)\n",
    "            speakers = speaker_dict[scene_id]\n",
    "            answers = []\n",
    "            for item in annotations:\n",
    "                query = (item['query']['sentenceIndex'], item['query']['startToken'], item['query']['endToken'])\n",
    "                antecedents = item['antecedents']\n",
    "                if antecedents in [['n', 'o', 't', 'P', 'r', 'e', 's', 'e', 'n', 't'], ['null_projection'], ['empty_subtitle']]:\n",
    "                    answers.append([query, \"notPresent\"])\n",
    "                else:\n",
    "                    temp_answer = []\n",
    "                    for antecedent in antecedents:\n",
    "                        if isinstance(antecedent, dict):\n",
    "                            temp_answer.append((antecedent['sentenceIndex'], antecedent['startToken'], antecedent['endToken']))\n",
    "                        else:\n",
    "                            temp_answer = \" \".join(antecedents)\n",
    "                    answers.append([query, temp_answer])\n",
    "            # mention_id_cluster = []\n",
    "            # for cluster in cluster_en_mentions_with_id(answers, sentences):\n",
    "            #     temp = []\n",
    "            #     for mention in cluster:\n",
    "            #         temp.append(tuple_mention_dict[mention])\n",
    "            #     mention_id_cluster.append(temp)\n",
    "            # en_mention_id_clusters[scene_id] = mention_id_cluster\n",
    "\n",
    "\n",
    "            zh_data.append({\n",
    "                \"sentences\": sentences,\n",
    "                \"answers\": answers,\n",
    "                \"speakers\": speakers,\n",
    "                \"scene_id\": scene_id,\n",
    "                \"mention_id_cluster\": mention_id_cluster\n",
    "            })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sentences': [['将', '光', '子', '正', '对', '平', '面', '上', '的', '双', '缝', '观', '察', '任', '意', '一', '个', '隙', '缝', '它', '不', '会', '穿', '过', '那', '两', '个', '隙', '缝', '如', '果', '没', '被', '观', '察', '那', '就', '会', '总', '之', '如', '果', '观', '察', '它', '在', '离', '开', '平', '面', '到', '击', '中', '目', '标', '之', '前', '它', '就', '不', '会', '穿', '过', '那', '两', '个', '隙', '缝'], ['没', '错', '但', '你', '为', '什', '么', '要', '说', '这', '个', '?'], ['没', '什', '么', '我', '只', '是', '觉', '得', '这', '个', '主', '意', '可', '以', '用', '于', '设', '计', 'T', '恤', '衫'], [], [], ['横', '1', '是', 'A', 'e', 'g', 'e', 'a', 'n', '竖', '8', '是', 'N', 'a', 'b', 'o', 'k', 'o', 'v', '横', '2', '6', '是', 'M', 'C', 'M', '竖', '1', '4', '是', '.', '.', '.', '手', '指', '挪', '开', '点', '这', '样', '一', '来', '横', '1', '4', '就', '是', 'P', 'o', 'r', 't', '瞧', '提', '示', '是', '\"', 'P', 'a', 'p', 'a', 'd', 'o', 'c', '的', '首', '都', '\"', '所', '以', '是', '太', '子', '港', '海', '地', '的'], ['能', '为', '你', '效', '劳', '吗', '?'], ['这', '里', '是', '高', '智', '商', '精', '子', '银', '行', '吗', '?'], ['如', '果', '你', '这', '么', '问', '也', '许', '你', '不', '该', '来', '这'], ['我', '想', '就', '是', '这', '没', '错', '了'], ['把', '这', '个', '填', '一', '填'], ['谢', '谢', '我', '们', '马', '上', '好'], ['慢', '慢', '来', '我', '还', '要', '玩', '填', '字', '游', '戏', '噢', '慢', '着'], ['我', '办', '不', '到'], ['开', '玩', '笑', '?', '你', '可', '是', '半', '职', '业', '人', '士'], ['不', '我', '们', '这', '样', '是', '诈', '骗', '我', '们', '没', '法', '保', '证', '生', '出', '来', '的', '一', '定', '是', '高', '智', '商', '小', '孩', '我', '姐', '姐', '跟', '我', '有', '一', '套', '相', '同', '的', '基', '本', '基', '因', '她', '却', '在', 'F', 'u', 'd', 'd', 'r', 'u', 'c', 'k', 'e', 'r', '餐', '厅', '当', '服', '务', '生'], ['这', '可', '是', '你', '的', '主', '意', '啊', '轻', '松', '赚', '点', '钱', '就', '有', '钱', '能', '升', '级', '我', '们', '的', '网', '络', '带', '宽'], ['我', '知', '道', '我', '确', '实', '很', '渴', '望', '高', '速', '下', '载', '但', '一', '些', '可', '怜', '的', '女', '人', '会', '把', '希', '望', '寄', '托', '在', '我', '的', '精', '子', '上', '万', '一', '生', '出', '来', '一', '个', '连', '曲', '线', '下', '部', '的', '面', '积', '用', '积', '分', '还', '是', '微', '分', '算', '都', '搞', '不', '清', '楚', '的', '小', '屁', '孩', '怎', '么', '办', '?'], ['我', '想', '她', '还', '是', '会', '爱', '那', '个', '宝', '宝', '的'], ['我', '不', '会'], ['你', '想', '要', '怎', '样', '?'], ['我', '想', '要', '走'], ['离', '开', '时', '要', '怎', '么', '说', '呢', '?'], ['我', '不', '知', '道', '我', '可', '从', '没', '有', '拒', '绝', '过', '提', '供', '精', '子', '的', '要', '求'], [], [], [], [], []], 'answers': [[(0, 1, 3), 'notPresent'], [(0, 5, 7), 'notPresent'], [(0, 9, 19), 'notPresent'], [(0, 13, 15), [(0, 1, 3)]], [(0, 19, 20), [(0, 13, 15)]], [(0, 25, 27), [(0, 9, 19)]], [(0, 35, 36), [(0, 19, 20)]], [(0, 44, 45), [(0, 35, 36)]], [(0, 48, 50), [(0, 5, 7)]], [(0, 51, 55), 'notPresent'], [(0, 57, 58), 'notPresent'], [(0, 64, 68), [(0, 9, 19)]], [(1, 4, 7), 'notPresent'], [(1, 3, 11), 'notPresent'], [(1, 3, 4), 'Sheldon'], [(2, 0, 3), [(1, 7, 11)]], [(2, 3, 4), 'Sheldon'], [(2, 8, 10), [(0, 1, 68)]], [(2, 10, 12), [(2, 8, 10)]], [(2, 18, 21), 'notPresent'], [(5, 3, 9), 'notPresent'], [(5, 12, 19), 'notPresent'], [(5, 23, 26), 'notPresent'], [(5, 33, 35), 'Receptionist'], [(5, 37, 38), 'notPresent'], [(5, 47, 51), 'notPresent'], [(5, 52, 66), [(5, 47, 51)]], [(5, 67, 69), [(5, 52, 66)]], [(5, 70, 75), [(5, 52, 66)]], [(5, 70, 73), 'notPresent'], [(6, 2, 3), 'Leonard'], [(7, 0, 2), 'notPresent'], [(7, 3, 10), [(7, 0, 2)]], [(8, 2, 3), 'Leonard'], [(8, 8, 9), 'Leonard'], [(9, 0, 1), 'Sheldon'], [(9, 4, 5), [(7, 3, 10)]], [(10, 1, 3), 'notPresent'], [(11, 2, 4), 'Sheldon'], [(11, 2, 4), 'Leonard'], [(12, 0, 3), 'notPresent'], [(12, 3, 4), 'Receptionist'], [(12, 7, 11), 'notPresent'], [(13, 0, 1), 'Sheldon'], [(14, 4, 5), 'Sheldon'], [(14, 7, 12), 'notPresent'], [(15, 1, 3), 'Leonard'], [(15, 1, 3), 'Sheldon'], [(15, 6, 8), 'notPresent'], [(15, 26, 27), 'notPresent'], [(15, 26, 27), 'Sheldon'], [(15, 26, 27), 'Leonard'], [(15, 21, 26), 'notPresent'], [(15, 30, 31), 'Sheldon'], [(15, 27, 34), 'notPresent'], [(15, 32, 41), 'notPresent'], [(15, 41, 42), [(15, 27, 29)]], [(15, 44, 54), 'notPresent'], [(16, 0, 1), [(15, 6, 8)]], [(16, 3, 7), [(16, 0, 1)]], [(16, 3, 4), 'Sheldon'], [(16, 10, 16), 'notPresent'], [(16, 17, 26), 'notPresent'], [(16, 21, 22), 'notPresent'], [(17, 0, 1), 'Sheldon'], [(17, 3, 4), 'Sheldon'], [(17, 9, 13), 'notPresent'], [(17, 14, 21), 'notPresent'], [(17, 22, 23), [(17, 16, 21)]], [(17, 22, 25), 'notPresent'], [(17, 28, 29), 'Sheldon'], [(17, 28, 32), 'notPresent'], [(17, 62, 65), 'notPresent'], [(17, 61, 62), [(17, 62, 65)]], [(17, 49, 51), 'notPresent'], [(17, 46, 48), 'notPresent'], [(17, 41, 43), 'notPresent'], [(18, 0, 1), 'Leonard'], [(18, 2, 3), [(17, 22, 23)]], [(18, 7, 9), [(17, 62, 65)]], [(19, 0, 1), 'Sheldon'], [(20, 0, 1), 'Sheldon'], [(21, 0, 1), 'Sheldon'], [(22, 2, 7), 'notPresent'], [(23, 0, 1), 'Leonard'], [(23, 4, 5), 'Leonard'], [(23, 12, 19), 'notPresent'], [(23, 14, 16), [(17, 30, 32)]], [(0, 5, 19), 'notPresent']], 'speakers': ['Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Receptionist', 'Leonard', 'Receptionist', 'Leonard', 'Receptionist', 'Sheldon', 'Receptionist', 'Leonard', 'Receptionist', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Receptionist', 'Sheldon', 'Leonard'], 'scene_id': 's01e01c00t0', 'mention_id_cluster': []}, {'sentences': [['你', '还', '在', '为', '精', '子', '银', '行', '的', '事', '生', '气', '吗', '?', '没', '有'], [], ['你', '想', '听', '一', '件', '关', '于', '楼', '梯', '的', '趣', '事', '吗', '?'], [], ['如', '果', '一', '阶', '楼', '梯', '比', '普', '通', '的', '矮', '个', '2', '毫', '米', '大', '部', '分', '人', '都', '会', '绊', '倒'], ['不', '关', '我', '事', '2', '毫', '米', '?'], ['不', '会', '吧', '是', '真', '的', '我', '1', '2', '岁', '时', '做', '了', '一', '系', '列', '的', '实', '验', '我', '爸', '爸', '因', '此', '还', '跌', '断', '了', '锁', '骨'], ['所', '以', '他', '们', '把', '你', '送', '去', '寄', '宿', '学', '校', '?'], ['不', '那', '是', '因', '为', '我', '研', '究', '激', '光', '惹', '的', '事'], ['新', '邻', '居', '?'], [], ['比', '上', '一', '任', '有', '了', '明', '显', '可', '观', '的', '改', '善'], ['那', '个', '2', '0', '0', '磅', '重', '还', '患', '有', '皮', '肤', '病', '的', '异', '装', '癖', '?'], [], [], [], [], ['我', '们', '不', '想', '打', '扰', '你', '我', '们', '是', '住', '对', '面', '的', '邻', '居'], ['啊', '太', '好', '了'], ['噢', '不', '我', '们', '不', '是', '同', '居', '我', '是', '说', '虽', '然', '住', '在', '一', '起', '但', '是', '在', '不', '同', '的', '.', '.', '.', '直', '男', '房', '间', '里'], ['那', '好', '吧', '我', '是', '你', '们', '的', '新', '邻', '居', '我', '叫', 'P', 'e', 'n', 'n', 'y'], [], [], [], [], ['欢', '迎', '你', '搬', '来'], [], [], [], [], [], [], [], [], ['我', '们', '应', '该', '邀', '请', '她', '共', '进', '午', '餐', '吗', '?'], ['不', '我', '们', '要', '看', 'B', 'a', 't', 't', 'l', 'e', 's', 't', 'a', 'r', 'G', 'a', 'l', 'a', 'c', 't', 'i', 'c', 'a', '第', '二', '季'], ['我', '们', '已', '经', '看', '过', '第', '二', '季', '的', 'D', 'V', 'D', '了'], ['那', '时', '候', '没', '带', '评', '论', '音', '轨', '一', '起', '看', '啊'], ['我', '们', '应', '该', '做', '好', '邻', '居', '请', '她', '过', '来', '让', '她', '感', '受', '到', '我', '们', '对', '她', '的', '欢', '迎'], ['我', '们', '从', '来', '没', '请', '那', '个', 'L', 'o', 'u', 'i', 's', '还', '不', 'L', 'o', 'u', 'i', 's', 'e', '过', '来', '啊'], ['那', '是', '我', '们', '不', '对', '我', '们', '要', '扩', '大', '社', '交', '圈'], ['我', '的', '社', '交', '圈', '很', '广', '我', '在', 'M', 'y', 'S', 'p', 'a', 'c', 'e', '上', '有', '2', '1', '2', '个', '朋', '友'], ['是', '的', '但', '你', '从', '没', '见', '过', '任', '何', '一', '个', '真', '人'], ['这', '正', '是', '它', '的', '美', '好', '之', '处'], ['我', '去', '请', '她', '过', '来', '我', '们', '好', '好', '吃', '一', '餐', '闲', '聊', '一', '下'], ['我', '们', '不', '闲', '聊', '至', '少', '不', '线', '下', '闲', '聊'], ['不', '会', '很', '难', '的', '你', '就', '听', '她', '说', '然', '后', '给', '一', '些', '适', '当', '的', '回', '应', '就', '好', '了'], ['直', '到', '什', '么', '时', '候', '?'], [], [], [], [], ['我', '们', '买', '了', '些', '印', '度', '菜', '那', '个', '.', '.', '.', '我', '知', '道', '搬', '家', '会', '很', '累', '人', '此', '外', '咖', '喱', '是', '天', '然', '的', '温', '和', '泻', '药', '你', '也', '知', '道', '干', '净', '的', '结', '肠', '会', '让', '你', '省', '去', '件', '烦', '心', '事'], ['虽', '然', '我', '不', '是', '专', '家', '但', '我', '相', '信', '当', '你', '邀', '请', '人', '吃', '午', '饭', '的', '时', '候', '最', '好', '省', '略', '关', '于', '肠', '部', '运', '动', '的', '话', '题'], ['你', '们', '是', '在', '邀', '我', '一', '起', '吃', '饭', '?'], [], ['真', '是', '太', '好', '了', '我', '很', '愿', '意'], [], ['你', '们', '平', '时', '都', '玩', '什', '么', '呢', '?'], ['今', '天', '我', '们', '尝', '试', '了', '靠', '手', '淫', '挣', '钱']], 'answers': [[(0, 0, 1), 'Leonard'], [(0, 4, 8), 'notPresent'], [(2, 0, 1), 'Leonard'], [(2, 3, 12), 'notPresent'], [(2, 7, 9), 'notPresent'], [(4, 10, 12), 'notPresent'], [(4, 2, 4), 'notPresent'], [(4, 10, 15), [(4, 10, 12)]], [(4, 15, 19), 'notPresent'], [(5, 0, 4), 'Leonard'], [(5, 4, 7), [(4, 12, 15)]], [(6, 13, 16), 'notPresent'], [(6, 17, 19), 'notPresent'], [(6, 6, 7), 'Leonard'], [(6, 19, 20), 'Sheldon'], [(6, 19, 22), 'notPresent'], [(6, 28, 30), 'notPresent'], [(7, 2, 4), 'notPresent'], [(7, 5, 6), 'Sheldon'], [(7, 8, 12), 'notPresent'], [(8, 1, 2), [(7, 5, 12)]], [(8, 2, 5), [(7, 5, 12)]], [(8, 5, 6), 'Sheldon'], [(8, 5, 8), 'notPresent'], [(8, 8, 10), 'notPresent'], [(9, 0, 3), 'notPresent'], [(11, 0, 4), 'notPresent'], [(12, 2, 17), [(11, 0, 4)]], [(12, 8, 13), 'notPresent'], [(12, 0, 2), [(9, 0, 3)]], [(17, 0, 2), 'Leonard'], [(17, 0, 2), 'Sheldon'], [(17, 7, 9), 'Leonard'], [(17, 7, 9), 'Sheldon'], [(17, 11, 13), 'notPresent'], [(19, 2, 4), 'Leonard'], [(19, 2, 4), 'Sheldon'], [(19, 8, 9), 'Leonard'], [(19, 20, 30), 'notPresent'], [(20, 3, 4), 'Penny'], [(20, 5, 7), 'Leonard'], [(20, 5, 7), 'Sheldon'], [(20, 5, 11), [(20, 3, 4)]], [(25, 3, 4), 'notPresent'], [(34, 0, 2), 'Leonard'], [(34, 0, 2), 'Sheldon'], [(34, 6, 7), 'Penny'], [(34, 7, 11), 'notPresent'], [(35, 1, 3), 'Leonard'], [(35, 1, 3), 'Sheldon'], [(35, 24, 27), 'notPresent'], [(35, 5, 24), 'notPresent'], [(36, 0, 2), 'Leonard'], [(36, 0, 2), 'Sheldon'], [(36, 6, 13), [(35, 24, 27)]], [(37, 5, 7), 'notPresent'], [(38, 0, 2), 'Sheldon'], [(38, 0, 2), 'Leonard'], [(38, 9, 10), [(34, 6, 7)]], [(38, 13, 14), [(38, 9, 10)]], [(39, 0, 2), 'Sheldon'], [(39, 0, 2), 'Leonard'], [(39, 8, 21), [(12, 2, 17)]], [(40, 2, 4), 'Leonard'], [(40, 2, 4), 'Sheldon'], [(40, 2, 4), 'Leonard'], [(40, 2, 4), 'Sheldon'], [(40, 11, 14), 'notPresent'], [(41, 0, 1), 'Sheldon'], [(41, 2, 7), [(40, 11, 14)]], [(41, 7, 8), 'Sheldon'], [(41, 18, 24), 'notPresent'], [(41, 9, 16), 'notPresent'], [(42, 3, 4), 'Sheldon'], [(42, 12, 14), [(41, 18, 24)]], [(43, 0, 1), [(42, 4, 14)]], [(43, 3, 4), [(42, 3, 14)]], [(44, 0, 1), 'Leonard'], [(44, 3, 4), [(38, 9, 10)]], [(44, 6, 8), 'Leonard'], [(44, 6, 8), [(38, 9, 10)]], [(44, 8, 13), 'notPresent'], [(44, 13, 15), 'notPresent'], [(45, 3, 5), [(44, 13, 15)]], [(45, 0, 2), 'Sheldon'], [(45, 0, 2), 'Leonard'], [(46, 5, 6), 'Sheldon'], [(46, 8, 9), [(44, 3, 4)]], [(46, 13, 15), 'notPresent'], [(46, 18, 20), 'notPresent'], [(47, 2, 4), 'notPresent'], [(47, 2, 4), 'notPresent'], [(52, 0, 2), 'Leonard'], [(52, 0, 2), 'Sheldon'], [(52, 5, 8), 'notPresent'], [(52, 13, 14), 'Leonard'], [(52, 45, 46), 'Leonard'], [(52, 30, 32), 'notPresent'], [(52, 24, 36), [(52, 5, 8)]], [(52, 34, 35), 'Penny'], [(52, 38, 43), 'notPresent'], [(52, 46, 52), 'notPresent'], [(53, 2, 3), 'Sheldon'], [(53, 3, 7), 'notPresent'], [(53, 11, 22), 'notPresent'], [(53, 13, 19), 'notPresent'], [(53, 12, 13), 'Leonard'], [(53, 33, 35), 'notPresent'], [(53, 28, 32), [(52, 38, 52)]], [(54, 0, 2), 'Leonard'], [(54, 5, 6), 'Penny'], [(56, 5, 6), 'Penny'], [(58, 6, 8), 'notPresent'], [(58, 0, 2), 'Leonard'], [(58, 0, 2), 'Sheldon'], [(58, 0, 2), 'Leonard'], [(58, 0, 2), 'Sheldon'], [(59, 2, 4), 'Sheldon'], [(59, 2, 4), 'Leonard'], [(59, 10, 12), 'notPresent'], [(52, 24, 26), 'notPresent'], [(52, 27, 34), 'notPresent']], 'speakers': ['Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Penny', 'Leonard', 'Sheldon', 'Penny', 'Leonard', 'Penny', 'Leonard', 'Penny', 'Leonard', 'Leonard', 'Sheldon', 'Penny', 'Leonard', 'Penny', 'Leonard', 'Penny', 'Sheldon', 'Leonard', 'Penny', 'Sheldon', 'Leonard', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Penny', 'Leonard', 'Sheldon', 'Penny', 'Leonard', 'Penny', 'Leonard', 'Penny', 'Sheldon'], 'scene_id': 's01e01c01t0', 'mention_id_cluster': []}, {'sentences': [[], [], ['不', '客', '气', '你', '只', '要', '向', '右', '.', '.', '.', '好', '我', '撤', '了'], [], ['那', '些', '洗', '发', '产', '品', '是', 'S', 'h', 'e', 'l', 'd', 'o', 'n', '的'], ['好', '的', '我', '能', '麻', '烦', '你', '帮', '个', '忙', '吗', '?', '帮', '忙', '?'], ['你', '当', '然', '可', '以', '要', '我', '帮', '忙', '我', '很', '乐', '意', '帮', '你', '忙'], ['你', '不', '答', '应', '也', '没', '事'], ['噢', '我', '应', '该', '会', '答', '应', '的'], ['这', '不', '是', '那', '种', '你', '会', '请', '刚', '认', '识', '的', '人', '帮', '的', '忙'], []], 'answers': [[(2, 3, 4), 'Penny'], [(4, 0, 6), 'notPresent'], [(4, 7, 14), 'notPresent'], [(5, 2, 3), 'Penny'], [(5, 6, 7), 'Leonard'], [(5, 9, 10), 'notPresent'], [(5, 12, 14), [(5, 9, 10)]], [(6, 0, 1), 'Penny'], [(6, 6, 7), 'Leonard'], [(6, 7, 9), [(5, 9, 10)]], [(6, 9, 10), 'Leonard'], [(6, 13, 16), [(5, 9, 10)]], [(6, 13, 16), 'Penny'], [(7, 0, 1), 'Leonard'], [(8, 1, 2), 'Leonard'], [(9, 0, 1), [(5, 9, 10)]], [(9, 15, 16), [(6, 7, 9)]], [(9, 5, 6), 'Penny'], [(9, 12, 13), 'Leonard'], [(9, 11, 12), 'Penny'], [(4, 7, 15), 'notPresent']], 'speakers': ['Leonard', 'Penny', 'Leonard', 'Penny', 'Leonard', 'Penny', 'Leonard', 'Penny', 'Leonard', 'Penny', 'Leonard'], 'scene_id': 's01e01c03t0', 'mention_id_cluster': []}, {'sentences': [['我', '们', '得', '回', '顾', '下', '这', '起', '事', '件', '的', '前', '因', '后', '果'], ['一', '定', '要', '吗', '?'], ['事', '件', 'A', ':', '一', '个', '美', '女', '在', '我', '们', '的', '浴', '室', '裸', '体', '事', '件', 'B', ':', '我', '们', '开', '车', '穿', '越', '整', '个', '小', '镇', '就', '为', '了', '搬', '台', '电', '视', '机', '回', '来', '目', '的', '地', '就', '在', '刚', '刚', '提', '到', '的', '美', '女', '的', '前', '男', '友', '家', '提', '问', ':'], ['因', '为', 'P', 'e', 'n', 'n', 'y', '请', '我', '帮', '个', '忙'], ['是', '啊', '那', '也', '许', '是', '最', '近', '似', '可', '能', '的', '起', '因', '了', '但', '我', '们', '都', '知', '道', '它', '和', '更', '高', '级', '的', '末', '端', '起', '因', '截', '然', '不', '同'], [], ['你', '用', '你', '的', '老', '二', '思', '考'], ['这', '在', '生', '理', '上', '是', '不', '可', '能', '的', '你', '也', '不', '一', '定', '要', '来', '啊'], ['对', '我', '可', '以', '留', '在', '家', '里', '看', '着', 'W', 'o', 'l', 'o', 'w', 'i', 't', 'z', '试', '图', '用', '俄', '语', '阿', '拉', '伯', '语', '及', '波', '斯', '语', '搭', '讪', 'P', 'e', 'n', 'n', 'y', '她', '为', '什', '么', '不', '能', '自', '己', '去', '拿', '?'], ['你', '知', '道', '分', '手', '这', '种', '事', '啦'], ['不', '我', '不', '知', '道', '你', '也', '不', '知', '道'], ['什', '么', '啊', '我', '跟', 'J', 'o', 'y', 'c', 'e', 'K', 'i', 'm', '分', '过', '手'], ['你', '才', '没', '有', '和', 'J', 'o', 'y', 'c', 'e', 'K', 'i', 'm', '分', '手', '是', '她', '跑', '到', '韩', '国', '去', '了'], ['为', '了', '修', '复', '她', '受', '伤', '的', '心', '灵', '现', '在', '这', '情', '况', '更', '加', '复', '杂', 'P', 'e', 'n', 'n', 'y', '和', '她', '前', '男', '友', '不', '能', '达', '成', '一', '致', '究', '竟', '电', '视', '机', '该', '归', '谁', '她', '不', '想', '再', '和', '他', '当', '面', '争', '执'], ['所', '以', '我', '们', '去', '和', '他', '当', '面', '争', '执', '?'], ['根', '本', '不', '会', '有', '争', '执', '我', '们', '两', '人', '他', '一', '人'], ['我', '们', '俩', '加', '起', '来', '都', '抗', '不', '动', '一', '台', '电', '视']], 'answers': [[(0, 0, 2), 'notPresent'], [(0, 11, 15), 'notPresent'], [(2, 0, 3), 'notPresent'], [(2, 4, 8), 'notPresent'], [(2, 9, 11), 'Sheldon'], [(2, 9, 11), 'Leonard'], [(2, 9, 14), 'notPresent'], [(2, 16, 19), 'notPresent'], [(2, 20, 22), 'Sheldon'], [(2, 20, 22), 'Leonard'], [(2, 28, 30), 'notPresent'], [(2, 33, 38), 'notPresent'], [(2, 47, 56), 'notPresent'], [(2, 53, 56), 'notPresent'], [(2, 40, 43), 'notPresent'], [(2, 43, 59), 'notPresent'], [(3, 8, 9), 'Leonard'], [(3, 9, 11), [(2, 4, 8)]], [(3, 11, 12), [(2, 20, 38)]], [(3, 2, 7), 'Sheldon'], [(4, 2, 3), [(3, 11, 12)]], [(4, 7, 14), [(3, 11, 12)]], [(4, 16, 18), 'Sheldon'], [(4, 16, 18), 'Leonard'], [(4, 18, 19), 'Sheldon'], [(4, 18, 19), 'Leonard'], [(4, 21, 22), 'notPresent'], [(4, 23, 31), 'notPresent'], [(6, 0, 1), 'Leonard'], [(6, 2, 6), 'notPresent'], [(6, 2, 3), 'Leonard'], [(7, 2, 9), 'notPresent'], [(7, 10, 11), 'Sheldon'], [(8, 1, 2), 'Sheldon'], [(8, 10, 18), 'notPresent'], [(8, 33, 38), [(2, 4, 8)]], [(8, 21, 23), 'notPresent'], [(8, 23, 27), 'notPresent'], [(8, 28, 31), 'notPresent'], [(8, 38, 39), [(8, 33, 38)]], [(8, 42, 46), [(2, 33, 38)]], [(9, 0, 1), 'Sheldon'], [(9, 3, 5), 'notPresent'], [(10, 1, 2), 'Sheldon'], [(10, 5, 6), 'Leonard'], [(11, 0, 2), 'Leonard'], [(11, 3, 4), 'Leonard'], [(11, 5, 13), 'notPresent'], [(12, 0, 1), 'Leonard'], [(12, 5, 13), [(11, 5, 13)]], [(12, 16, 17), [(11, 5, 13)]], [(12, 19, 21), 'notPresent'], [(13, 4, 10), 'notPresent'], [(13, 4, 5), [(12, 5, 13)]], [(13, 12, 15), [(9, 3, 5)]], [(13, 33, 35), 'notPresent'], [(13, 19, 24), [(8, 33, 38)]], [(13, 25, 29), [(2, 53, 56)]], [(13, 25, 26), [(8, 33, 38)]], [(13, 26, 29), [(2, 53, 56)]], [(13, 42, 43), [(13, 19, 24)]], [(13, 42, 43), [(13, 25, 29)]], [(13, 37, 40), 'notPresent'], [(13, 43, 44), [(13, 19, 24)]], [(13, 49, 53), 'notPresent'], [(13, 48, 49), [(13, 26, 29)]], [(14, 2, 4), 'Sheldon'], [(14, 2, 4), 'Leonard'], [(14, 9, 11), [(13, 49, 53)]], [(14, 6, 7), [(13, 26, 29)]], [(15, 5, 7), [(14, 9, 11)]], [(15, 7, 9), 'Leonard'], [(15, 7, 9), 'Sheldon'], [(15, 11, 12), [(13, 26, 29)]], [(16, 0, 2), 'Sheldon'], [(16, 0, 2), 'Leonard'], [(16, 12, 14), 'notPresent'], [(4, 16, 19), 'Sheldon'], [(4, 16, 19), 'Leonard'], [(13, 19, 29), 'notPresent'], [(13, 25, 29), 'notPresent'], [(15, 0, 2), 'Sheldon'], [(16, 0, 4), 'Sheldon'], [(16, 0, 4), 'Leonard']], 'speakers': ['Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon', 'Leonard', 'Sheldon'], 'scene_id': 's01e01c04t0', 'mention_id_cluster': []}, {'sentences': [['你', '们', '跟', 'L', 'e', 'o', 'n', 'a', 'r', 'd', '和', 'S', 'h', 'e', 'l', 'd', 'o', 'n', '一', '起', '在', '学', '校', '做', '事', '?'], ['不', '好', '意', '思', '你', '会', '说', '英', '语', '吗', '?'], ['他', '会', '只', '是', '不', '能', '跟', '女', '人', '说'], [], ['因', '为', '他', '是', '个', '书', '呆', '子', '盒', '装', '果', '汁', '?']], 'answers': [[(0, 0, 2), 'notPresent'], [(0, 0, 2), 'notPresent'], [(0, 3, 10), 'notPresent'], [(0, 11, 18), 'notPresent'], [(0, 21, 23), 'notPresent'], [(1, 4, 5), [(0, 0, 2)]], [(1, 7, 9), 'notPresent'], [(2, 0, 2), [(1, 4, 5)]], [(2, 7, 9), 'notPresent'], [(4, 2, 3), [(2, 0, 2)]], [(4, 4, 8), [(2, 0, 2)]], [(4, 8, 12), 'notPresent'], [(0, 3, 18), 'notPresent'], [(4, 4, 8), [(4, 2, 3)]]], 'speakers': ['Penny', 'Penny', 'Howard', 'Penny', 'Howard'], 'scene_id': 's01e01c05t0', 'mention_id_cluster': []}]\n"
     ]
    }
   ],
   "source": [
    "print(zh_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 21, 22), (0, 8, 9), (0, 1, 2), (0, 27, 28), (0, 11, 12)], [(0, 3, 4), (0, 30, 31)], [(0, 39, 41), (0, 6, 11), (0, 15, 16)], [(0, 32, 33)], [(0, 34, 35)], [(1, 3, 4)], [(1, 2, 6)], [(2, 0, 1), (1, 4, 6)], [(0, 1, 41), (2, 4, 5), (2, 5, 6)], [(2, 9, 10)], [(5, 3, 4)], [(5, 7, 8)], [(5, 11, 12)], [(5, 18, 19)], [(5, 23, 24), (5, 25, 31), (5, 34, 36), (5, 32, 33)], [(5, 34, 35)], [(7, 0, 1)], [(7, 2, 5), (9, 3, 4)], [(10, 1, 2)], [(12, 0, 1)], [(12, 4, 5)], [(14, 4, 7)], [(16, 0, 1), (15, 4, 5)], [(15, 15, 16)], [(15, 13, 15)], [(15, 16, 21)], [(15, 20, 25)], [(15, 25, 26), (15, 16, 17)], [(15, 28, 29)], [(16, 2, 5)], [(16, 7, 11)], [(16, 12, 16)], [(16, 14, 15)], [(17, 6, 7)], [(17, 8, 12)], [(17, 13, 14), (17, 9, 12), (18, 2, 3)], [(17, 13, 15)], [(17, 17, 20)], [(17, 36, 37), (18, 5, 6), (17, 37, 38)], [(17, 30, 31)], [(17, 28, 29)], [(17, 25, 26)], [(22, 1, 4)], [(23, 9, 13)], [(17, 19, 20), (23, 10, 11)], [(0, 3, 11)]]\n",
      "[[(0, 4, 6)], [(2, 3, 8)], [(2, 5, 6)], [(4, 6, 7), (4, 6, 9)], [(4, 1, 2)], [(4, 9, 11)], [(4, 7, 9), (5, 1, 3)], [(6, 10, 11)], [(6, 12, 13)], [(6, 13, 15)], [(6, 19, 20)], [(7, 1, 2)], [(7, 6, 8)], [(7, 3, 8), (8, 2, 3), (8, 1, 2)], [(8, 3, 5)], [(8, 5, 6)], [(9, 0, 2)], [(11, 0, 2)], [(12, 1, 8), (39, 5, 9)], [(12, 4, 6)], [(12, 0, 1)], [(17, 7, 8)], [(19, 12, 18)], [(20, 4, 8), (20, 2, 3)], [(25, 2, 3)], [(34, 4, 5)], [(35, 5, 6), (36, 3, 6)], [(35, 4, 5)], [(37, 3, 4)], [(44, 3, 4), (38, 5, 6), (44, 5, 6), (46, 6, 7), (34, 3, 4), (38, 8, 9)], [(41, 2, 4), (40, 8, 9)], [(42, 8, 9), (41, 9, 12)], [(41, 6, 7)], [(43, 0, 1), (42, 4, 9)], [(43, 2, 3), (42, 3, 9)], [(44, 6, 9)], [(45, 2, 3), (44, 9, 10)], [(46, 10, 11)], [(46, 13, 14)], [(47, 1, 2)], [(52, 4, 6), (52, 15, 23)], [(52, 19, 20)], [(52, 24, 27)], [(52, 30, 33)], [(53, 2, 4)], [(53, 7, 15)], [(53, 9, 13)], [(53, 21, 22)], [(52, 24, 33), (53, 18, 20)], [(58, 4, 5)], [(59, 6, 7)], [(52, 15, 16)], [(52, 17, 21)]]\n",
      "[[(4, 0, 3)], [(4, 4, 5)], [(9, 12, 13), (6, 5, 6), (5, 10, 11), (5, 7, 8), (9, 0, 1), (6, 9, 10)], [(4, 4, 6)]]\n",
      "[[(0, 0, 1)], [(0, 7, 8)], [(2, 0, 2)], [(13, 13, 14), (14, 4, 5), (13, 22, 23), (2, 3, 5), (13, 11, 12), (13, 13, 15), (3, 4, 5), (13, 14, 15), (15, 6, 7), (8, 13, 14), (13, 23, 24), (13, 27, 28), (8, 14, 15), (2, 31, 32)], [(2, 6, 9)], [(2, 10, 12)], [(2, 17, 18)], [(2, 20, 22), (8, 16, 17)], [(2, 27, 32)], [(2, 23, 24)], [(2, 24, 34)], [(3, 5, 6), (2, 13, 22), (4, 6, 10), (4, 2, 3)], [(4, 15, 16)], [(4, 17, 22)], [(6, 2, 5)], [(7, 2, 7)], [(8, 6, 7)], [(8, 8, 9)], [(8, 9, 10)], [(8, 11, 12)], [(9, 2, 3), (13, 7, 9)], [(11, 4, 5), (12, 7, 8), (12, 4, 5), (13, 2, 3)], [(12, 10, 11)], [(13, 2, 6)], [(13, 17, 18)], [(13, 19, 20)], [(15, 3, 4), (13, 28, 30), (14, 6, 7)], [(16, 6, 7)], [(13, 11, 15)]]\n",
      "[[(0, 0, 1)], [(0, 2, 3)], [(0, 4, 5)], [(0, 7, 8)], [(2, 0, 1), (1, 1, 2)], [(1, 4, 5)], [(2, 4, 5)], [(4, 1, 2), (4, 3, 5)], [(4, 5, 7)], [(0, 2, 5)]]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "all_ids = []\n",
    "with open('data/raw_source/dialogue_zh/all_coref_data_en_zh_seg.json', 'r') as f:\n",
    "# with open('data/raw_source/dialogue_zh/dev-test-batch1_zh.json', 'r') as f:\n",
    "    reader = jsonlines.Reader(f)\n",
    "    for bulk in reader:\n",
    "        for idx, instance in enumerate(bulk):\n",
    "            if idx>=5:\n",
    "                break\n",
    "            scene_id = instance['scene_id']\n",
    "            if scene_id == \"\":\n",
    "                continue\n",
    "            sentences = instance['sentences']\n",
    "            # print(sentences)\n",
    "            # sentences = [[token for token in \"\".join(sent)] for sent in instance['sentences']]\n",
    "            annotations = instance['annotations']\n",
    "            all_ids.append(scene_id)\n",
    "            speakers = speaker_dict[scene_id]\n",
    "            answers = []\n",
    "            for item in annotations:\n",
    "                query = (item['query']['sentenceIndex'], item['query']['startToken'], item['query']['endToken'])\n",
    "                antecedents = item['antecedents']\n",
    "                # print(query)\n",
    "                # print(antecedents)\n",
    "                # print()\n",
    "                if antecedents in [['n', 'o', 't', 'P', 'r', 'e', 's', 'e', 'n', 't'], ['null_projection'], ['empty_subtitle']]:\n",
    "                    answers.append([query, \"notPresent\"])\n",
    "                else:\n",
    "                    temp_answer = []\n",
    "                    for antecedent in antecedents:\n",
    "                        if isinstance(antecedent, dict):\n",
    "                            temp_answer.append((antecedent['sentenceIndex'], antecedent['startToken'], antecedent['endToken']))\n",
    "                        else:\n",
    "                            temp_answer = \" \".join(antecedents)\n",
    "                    answers.append([query, temp_answer])\n",
    "\n",
    "            clusters = cluster_en_mentions_with_id(answers, sentences)\n",
    "            print(clusters)\n",
    "\n",
    "            data.append(remove_empty_sentences({\n",
    "                \"sentences\": sentences,\n",
    "                \"answers\": answers,\n",
    "                \"speakers\": speakers,\n",
    "                \"scene_id\": scene_id\n",
    "            }))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['是', 'S', 'h', 'e', 'l', 'd', 'o', 'n', '的', '信', '息', '他', '说', '他', '一', '会', '儿', '就', '到'], ['你', '在', '干', '吗', '?'], ['他', '要', '确', '认', '你', '车', '里', '没', '有', '空', '气', '清', '新', '剂', '才', '会', '出', '现'], ['你', '这', '周', '日', '子', '不', '好', '过', '了'], ['那', '还', '不', '是', '因', '为', '你'], ['我', '让', '你', '们', '不', '要', '互', '相', '揶', '揄', '又', '没', '让', '你', '带', '他', '去', '渡', '蜜', '月'], ['我', '都', '不', '清', '楚', '你', '说', '了', '什', '么', '该', '死', '你', '的', '大', '波', '和', '消', '音', '器', '一', '样'], ['回', '得', '州', '很', '开', '心', '吧', '?'], ['和', '一', '名', '真', '正', '的', '宇', '航', '员', '一', '起', '参', '观', '宇', '航', '局', '可', '不', '是', '天', '天', '都', '有', '的', '好', '事'], ['和', '哪', '个', '宇', '航', '员', '?'], ['奥', '尔', '德', '林']]\n",
      "\n",
      "[(0, 1, 8), 'notPresent']\n",
      "[(0, 11, 12), [(0, 1, 8)]]\n",
      "[(0, 13, 14), [(0, 11, 12)]]\n",
      "[(0, 14, 17), 'notPresent']\n",
      "[(1, 2, 4), 'notPresent']\n",
      "[(1, 0, 1), 'Howard']\n",
      "[(2, 0, 1), [(0, 1, 8)]]\n",
      "[(2, 2, 14), 'notPresent']\n",
      "[(2, 9, 14), 'notPresent']\n",
      "[(2, 4, 7), 'notPresent']\n",
      "[(2, 4, 5), 'Bernadette']\n",
      "[(3, 1, 3), 'notPresent']\n",
      "[(3, 3, 8), 'notPresent']\n",
      "[(3, 0, 1), 'Howard']\n",
      "[(4, 4, 7), 'notPresent']\n",
      "[(4, 6, 7), 'Howard']\n",
      "[(5, 0, 1), 'Bernadette']\n",
      "[(5, 6, 8), 'Howard']\n",
      "[(5, 6, 8), [(0, 1, 8)]]\n",
      "[(5, 13, 14), 'Howard']\n",
      "[(5, 15, 16), [(0, 1, 8)]]\n",
      "[(5, 16, 20), 'notPresent']\n",
      "[(6, 0, 1), 'Howard']\n",
      "[(6, 8, 10), 'notPresent']\n",
      "[(6, 5, 6), 'Bernadette']\n",
      "[(6, 12, 20), 'notPresent']\n",
      "[(6, 12, 13), 'Bernadette']\n",
      "[(6, 12, 20), 'notPresent']\n",
      "[(7, 0, 3), 'notPresent']\n",
      "[(8, 19, 21), 'notPresent']\n",
      "[(8, 13, 16), 'notPresent']\n",
      "[(8, 1, 9), 'notPresent']\n",
      "[(9, 1, 3), 'notPresent']\n",
      "[(9, 3, 6), [(8, 1, 9)]]\n",
      "[(10, 0, 4), [(9, 3, 6)]]\n",
      "\n",
      "['Howard', 'Bernadette', 'Howard', 'Bernadette', 'Howard', 'Bernadette', 'Howard', 'Bernadette', 'Howard', 'Sheldon', 'Howard']\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "correction_dict = {(5, 7, 8): (5, 7, 9), (7, 13, 17): (7, 15, 17), (11, 0, 1): (11, 0, 2), (11, 5, 7): (11, 4, 7), (16, 0, 3): (16, 0, 1)}\n",
    "for item in data:\n",
    "    if item['scene_id'] in ['s07e17c08t1']:\n",
    "        print(item['sentences'])\n",
    "        print()\n",
    "        for token in item['answers']:\n",
    "            print(token)\n",
    "        print()\n",
    "        print(item['speakers'])\n",
    "        print('=='*50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "split = \"test\"\n",
    "\n",
    "document = []\n",
    "for i in range(len(data)):\n",
    "    sample = data[i]\n",
    "    if sample['scene_id'] not in split_dict[split]:\n",
    "        continue\n",
    "    # print(sample)\n",
    "    # print()\n",
    "    # original_sentences = sample['sentences']\n",
    "    # original_clusters = cluster_mentions(sample['answers'], original_sentences)\n",
    "    # sentences, clusters, speakers = remove_speaker_prefix(original_sentences, original_clusters)\n",
    "    sentences = sample['sentences']\n",
    "    clusters = cluster_mentions(sample['answers'], sentences)\n",
    "    speakers = sample['speakers']\n",
    "    scene_id = sample['scene_id']\n",
    "    part = int(scene_id[7:9])\n",
    "    begin_line = \"#begin document \" + \"(\" + scene_id + \"); part \" + \"%03d\" % part\n",
    "    end_line = \"#end document\"\n",
    "\n",
    "    # Prepare for clustering\n",
    "    cluster_field = []\n",
    "    for sent in sentences:\n",
    "        cluster_field.append([\"\"]*len(sent))\n",
    "    # Add start\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                # print(cluster_field[sent_id])\n",
    "                # print(sent_id, start, end, len(sentences[sent_id]))\n",
    "                # print(sentences[sent_id])\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx)\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx)\n",
    "    # Add start==end\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start == end:\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx) + \")\"\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx) + \")\"\n",
    "    # Add End\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                try:\n",
    "                    if cluster_field[sent_id][end] == \"\":\n",
    "                        cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                    else:\n",
    "                        cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "                except:\n",
    "                    pass\n",
    "                # if cluster_field[sent_id][end] == \"\":\n",
    "                #     cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                # else:\n",
    "                #     cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "\n",
    "    # Build document\n",
    "    document.append(begin_line + \"\\n\")\n",
    "    for sent, speaker, cluster_value in zip(sentences, speakers, cluster_field):\n",
    "        for j, word in enumerate(sent):\n",
    "            cluster_id = cluster_value[j]\n",
    "            if cluster_id == \"\":\n",
    "                cluster_id = \"-\"\n",
    "            temp = [scene_id, str(part), str(j), word, \"na\", \"na\", \"na\", \"na\", \"na\", speaker, \"na\", \"na\", \"na\", cluster_id]\n",
    "            document.append(\" \".join(temp)+ \"\\n\")\n",
    "        document.append(\"\" + \"\\n\")\n",
    "    document.append(end_line + \"\\n\")\n",
    "\n",
    "with open(\"data/conll_style/dialogue_chinese/\"+ split+'.chinese.v4_gold_conll', 'w') as f:\n",
    "    f.writelines(document)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Chinese Sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1515\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "\n",
    "document = []\n",
    "for i in range(len(data[:1])):\n",
    "    sample = data[i]\n",
    "    if sample['scene_id'] not in split_dict[split]:\n",
    "        continue\n",
    "    # print(sample)\n",
    "    # print()\n",
    "    # original_sentences = sample['sentences']\n",
    "    # original_clusters = cluster_mentions(sample['answers'], original_sentences)\n",
    "    # sentences, clusters, speakers = remove_speaker_prefix(original_sentences, original_clusters)\n",
    "    sentences = sample['sentences']\n",
    "    clusters = cluster_mentions(sample['answers'], sentences)\n",
    "    speakers = sample['speakers']\n",
    "    scene_id = sample['scene_id']\n",
    "    part = int(scene_id[7:9])\n",
    "    begin_line = \"#begin document \" + \"(\" + scene_id + \"); part \" + \"%03d\" % part\n",
    "    end_line = \"#end document\"\n",
    "\n",
    "    # Prepare for clustering\n",
    "    cluster_field = []\n",
    "    for sent in sentences:\n",
    "        cluster_field.append([\"\"]*len(sent))\n",
    "    # Add start\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                # print(cluster_field[sent_id])\n",
    "                # print(sent_id, start, end, len(sentences[sent_id]))\n",
    "                # print(sentences[sent_id])\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx)\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx)\n",
    "    # Add start==end\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start == end:\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx) + \")\"\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx) + \")\"\n",
    "    # Add End\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                try:\n",
    "                    if cluster_field[sent_id][end] == \"\":\n",
    "                        cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                    else:\n",
    "                        cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "                except:\n",
    "                    pass\n",
    "                # if cluster_field[sent_id][end] == \"\":\n",
    "                #     cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                # else:\n",
    "                #     cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "\n",
    "    # Build document\n",
    "    document.append(begin_line + \"\\n\")\n",
    "    for sent, speaker, cluster_value in zip(sentences, speakers, cluster_field):\n",
    "        for j, word in enumerate(sent):\n",
    "            cluster_id = cluster_value[j]\n",
    "            if cluster_id == \"\":\n",
    "                cluster_id = \"-\"\n",
    "            temp = [scene_id, str(part), str(j), word, \"na\", \"na\", \"na\", \"na\", \"na\", speaker, \"na\", \"na\", \"na\", cluster_id]\n",
    "            document.append(\" \".join(temp)+ \"\\n\")\n",
    "        document.append(\"\" + \"\\n\")\n",
    "    document.append(end_line + \"\\n\")\n",
    "\n",
    "with open(\"data/conll_style/overfit_chinese/\"+ \"train\"+'.chinese.v4_gold_conll', 'w') as f:\n",
    "    f.writelines(document)\n",
    "with open(\"data/conll_style/overfit_chinese/\"+ \"dev\"+'.chinese.v4_gold_conll', 'w') as f:\n",
    "    f.writelines(document)\n",
    "with open(\"data/conll_style/overfit_chinese/\"+ \"test\"+'.chinese.v4_gold_conll', 'w') as f:\n",
    "    f.writelines(document)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44425\n"
     ]
    }
   ],
   "source": [
    "print(len(document))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "file_name = \"test\"\n",
    "data = []\n",
    "with open('data/'+ file_name+'_temp.pkl', 'rb') as f:\n",
    "    data.extend(pkl.load(f))\n",
    "\n",
    "document = []\n",
    "for i in range(len(data)):\n",
    "    if file_name==\"train\" and i==38:\n",
    "        continue\n",
    "    if file_name==\"test\" and i==28:\n",
    "        continue\n",
    "\n",
    "    # if i>=100:\n",
    "    #     continue\n",
    "    sample = data[i]\n",
    "    original_sentences = sample['sentences']\n",
    "    original_clusters = cluster_mentions(sample['answers'], original_sentences)\n",
    "\n",
    "    # Get Data ready for conversion\n",
    "    sentences, clusters, speakers = remove_speaker_prefix(original_sentences, original_clusters)\n",
    "    scene_id = sample['scene_id']\n",
    "    part = int(scene_id[7:9])\n",
    "    begin_line = \"#begin document \" + \"(\" + scene_id + \"); part \" + \"%03d\" % part\n",
    "    end_line = \"#end document\"\n",
    "\n",
    "    # Prepare for clustering\n",
    "    cluster_field = []\n",
    "    for sent in sentences:\n",
    "        cluster_field.append([\"\"]*len(sent))\n",
    "    # Add start\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx)\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx)\n",
    "    # Add start==end\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start == end:\n",
    "                if cluster_field[sent_id][start] == \"\":\n",
    "                    cluster_field[sent_id][start] += \"(\" + str(idx) + \")\"\n",
    "                else:\n",
    "                    cluster_field[sent_id][start] += \"|\" + \"(\" + str(idx) + \")\"\n",
    "    # Add End\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        for sent_id, start, end in cluster:\n",
    "            end = end - 1\n",
    "            if start != end:\n",
    "                try:\n",
    "                    if cluster_field[sent_id][end] == \"\":\n",
    "                        cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                    else:\n",
    "                        cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "                except:\n",
    "                    pass\n",
    "                # if cluster_field[sent_id][end] == \"\":\n",
    "                #     cluster_field[sent_id][end] += str(idx) + \")\"\n",
    "                # else:\n",
    "                #     cluster_field[sent_id][end] += \"|\" + str(idx) + \")\"\n",
    "\n",
    "    # Build document\n",
    "    document.append(begin_line + \"\\n\")\n",
    "    for sent, speaker, cluster_value in zip(sentences, speakers, cluster_field):\n",
    "        for j, word in enumerate(sent):\n",
    "            cluster_id = cluster_value[j]\n",
    "            if cluster_id == \"\":\n",
    "                cluster_id = \"-\"\n",
    "            temp = [scene_id, str(part), str(j), word, \"na\", \"na\", \"na\", \"na\", \"na\", speaker, \"na\", \"na\", \"na\", cluster_id]\n",
    "            document.append(\" \".join(temp)+ \"\\n\")\n",
    "        document.append(\"\" + \"\\n\")\n",
    "    document.append(end_line + \"\\n\")\n",
    "\n",
    "with open(\"data/input/\"+ file_name+'.english.v4_gold_conll', 'w') as f:\n",
    "    f.writelines(document)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}