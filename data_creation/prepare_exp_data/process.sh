#!/bin/bash

#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_english --seg_len 512 --language english --tokenizer_name SpanBERT/spanbert-base-cased
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/ontonotes_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/ontonotes_english --seg_len 512 --language english --tokenizer_name SpanBERT/spanbert-base-cased
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/friends_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/friends_english --seg_len 512 --language english --tokenizer_name SpanBERT/spanbert-base-cased
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/ontonotes_arabic --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/ontonotes_arabic --seg_len 512 --language arabic --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_farsi --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_farsi --seg_len 512 --language farsi --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/sample_uncorrected --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/sample_uncorrected --seg_len 512 --language chinese --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/overfit_chinese --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/overfit_chinese --seg_len 512 --language chinese --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/overfit_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/overfit_english --seg_len 512 --language english --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/overfit_farsi --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/overfit_farsi --seg_len 512 --language farsi --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_finalized_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_finalized_english --seg_len 512 --language english --tokenizer_name SpanBERT/spanbert-base-cased

#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_english --seg_len 512 --language english --tokenizer_name SpanBERT/spanbert-base-cased
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_xlmr_english --seg_len 512 --language english --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_chinese --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_chinese --seg_len 512 --language chinese --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_chinese --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_chinese --seg_len 512 --language chinese --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_farsi --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_farsi --seg_len 512 --language farsi --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/ci_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/ci_english --seg_len 512 --language english --tokenizer_name bert-large-cased
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_name_replaced_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_name_replaced_english --seg_len 512 --language english --tokenizer_name SpanBERT/spanbert-base-cased
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_name_replaced_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_name_replaced_xlmr_english --seg_len 512 --language english --tokenizer_name xlm-roberta-base

#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_name_replaced_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_name_replaced_chinese --seg_len 512 --language chinese --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_name_replaced_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_name_replaced_farsi --seg_len 512 --language chinese --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_name_replaced_2_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_name_replaced_2_english --seg_len 512 --language english --tokenizer_name bert-large-cased
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_name_replaced_3_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_name_replaced_3_english --seg_len 512 --language english --tokenizer_name bert-large-cased


#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_english --seg_len 512 --language english --tokenizer_name bert-large-cased
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_uncased_english --seg_len 512 --language english --tokenizer_name bert-large-uncased

#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/ontonotes_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/ontonotes_english --seg_len 512 --language english --tokenizer_name bert-large-cased
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_corrected_1_chinese --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_corrected_1_chinese --seg_len 512 --language chinese --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_all_corrected_chinese --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_all_corrected_chinese --seg_len 512 --language chinese --tokenizer_name xlm-roberta-base


#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_all_corrected_chinese --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/test --seg_len 512 --language chinese --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_chinese --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/test --seg_len 512 --language chinese --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/chinese_temp/ --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/temp --seg_len 512 --language chinese --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_prob_source_chinese/ --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_prob_source_chinese --seg_len 512 --language chinese --tokenizer_name xlm-roberta-base
#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/dialogue_corrected_farsi --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/dialogue_corrected_farsi --seg_len 512 --language farsi --tokenizer_name xlm-roberta-base


#python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/ontonotes_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/ontonotes_xlmr_english --seg_len 512 --language english --tokenizer_name xlm-roberta-base
python preprocess.py --input_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/conll_style/ci_english --output_dir /Users/person_name_1zheng/PycharmProjects/dialogue_processing/data/experiment_inputs/ci_xlmr_english --seg_len 512 --language english --tokenizer_name xlm-roberta-base
